[
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365603827",
        "pull_request_review_id": 341593048,
        "id": 365603827,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwMzgyNw==",
        "diff_hunk": "@@ -0,0 +1,55 @@\n+import click\n+\n+from robotstxtpy.crawler import Crawler\n+from robotstxtpy.utils import validate_url\n+\n+\n+@click.command()\n+@click.option('--agent', prompt='who is the user-agent',\n+              help='user agent')\n+def add_user_agent(agent):\n+    if agent.isspace():\n+        click.echo('No user agent added. Please enter a user_agent')\n+        add_user_agent()\n+    elif agent == 'exit':\n+        return\n+    else:\n+        click.echo('User agent is %s' % agent)\n+        url_input()\n+\n+\n+@click.command()\n+@click.option('--ans', prompt='more user-agents to add',\n+              help=\"'y' to add more user-agents 'n' to produce the robots.txt\")\n+def more_user_agent(ans):\n+    if ans == 'y':\n+        click.echo('y')\n+    elif ans == 'n':\n+        click.echo('n')\n+    else:\n+        click.echo('that is not a valid answer')\n+        more_user_agent()\n+\n+\n+@click.command()\n+@click.option('--url', prompt='Enter URL to generate robots.txt',\n+              help='Enter URL to be crawled (must be base path to website)')\n+def url_input(URL):\n+    if not URL:\n+        click.echo('Enter a URL!')\n+        url_input()\n+    elif not validate_url(URL):\n+        click.echo('Invalid URL')\n+        url_input()\n+\n+    crawler = Crawler(URL)\n+    urls = crawler.get_endpoints_from_url()\n+    click.echo(urls)\n+\n+# asks user for user agent",
        "path": "crawler-cli.py",
        "position": null,
        "original_position": 49,
        "commit_id": "4ccf93bbcbcf2af100a258a075b346847e66ca00",
        "original_commit_id": "fd7b9ffc07d52f8c02d3588a9a65a7c1c8a47e06",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove these comments please",
        "created_at": "2020-01-12T19:07:27Z",
        "updated_at": "2020-01-12T19:13:02Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/33#discussion_r365603827",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/33",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365603827"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/33#discussion_r365603827"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/33"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365603850",
        "pull_request_review_id": 341593048,
        "id": 365603850,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwMzg1MA==",
        "diff_hunk": "@@ -0,0 +1,55 @@\n+import click\n+\n+from robotstxtpy.crawler import Crawler\n+from robotstxtpy.utils import validate_url\n+\n+\n+@click.command()\n+@click.option('--agent', prompt='who is the user-agent',\n+              help='user agent')\n+def add_user_agent(agent):\n+    if agent.isspace():\n+        click.echo('No user agent added. Please enter a user_agent')\n+        add_user_agent()\n+    elif agent == 'exit':\n+        return\n+    else:\n+        click.echo('User agent is %s' % agent)\n+        url_input()\n+\n+\n+@click.command()\n+@click.option('--ans', prompt='more user-agents to add',\n+              help=\"'y' to add more user-agents 'n' to produce the robots.txt\")\n+def more_user_agent(ans):\n+    if ans == 'y':\n+        click.echo('y')\n+    elif ans == 'n':\n+        click.echo('n')\n+    else:\n+        click.echo('that is not a valid answer')\n+        more_user_agent()\n+\n+\n+@click.command()\n+@click.option('--url', prompt='Enter URL to generate robots.txt',\n+              help='Enter URL to be crawled (must be base path to website)')\n+def url_input(URL):\n+    if not URL:\n+        click.echo('Enter a URL!')\n+        url_input()\n+    elif not validate_url(URL):\n+        click.echo('Invalid URL')\n+        url_input()\n+\n+    crawler = Crawler(URL)\n+    urls = crawler.get_endpoints_from_url()\n+    click.echo(urls)\n+\n+# asks user for user agent\n+# asks user for URL\n+# generate robot.txt\n+\n+\n+if __name__ == '__main__':",
        "path": "crawler-cli.py",
        "position": null,
        "original_position": 54,
        "commit_id": "4ccf93bbcbcf2af100a258a075b346847e66ca00",
        "original_commit_id": "fd7b9ffc07d52f8c02d3588a9a65a7c1c8a47e06",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Also, remove this one",
        "created_at": "2020-01-12T19:07:48Z",
        "updated_at": "2020-01-12T19:13:02Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/33#discussion_r365603850",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/33",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365603850"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/33#discussion_r365603850"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/33"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365603938",
        "pull_request_review_id": 341593048,
        "id": 365603938,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwMzkzOA==",
        "diff_hunk": "@@ -0,0 +1,87 @@\n+import click\n+\n+from robotstxtpy import RobotsTxt\n+from robotstxtpy.crawler import Crawler\n+from robotstxtpy.utils import filter_out_path_from_url\n+\n+# from robotstxtpy.utils import validate_url\n+\n+user_agent_list = []\n+robottxt = RobotsTxt()\n+\n+\n+@click.command()\n+@click.option('--agent', prompt='who is the user-agent',\n+              help='user agent')\n+def add_user_agent(agent):\n+    if agent.isspace():\n+        click.echo('No user agent added. Please enter a user_agent')\n+        add_user_agent()\n+    elif agent == 'exit':\n+        return\n+    else:\n+        click.echo('User agent is %s' % agent)\n+        user_agent_list.append(agent)\n+        more_user_agent()\n+\n+\n+@click.command()\n+@click.option('--ans', prompt='Add more user-agents?(y/n)',\n+              help=\"'y' to add more user-agents 'n' to produce the robots.txt\")\n+def more_user_agent(ans):\n+    if ans == 'y':\n+        click.echo('y')\n+        add_user_agent()\n+    elif ans == 'n':\n+        click.echo('n')\n+        url_input()\n+    else:\n+        click.echo('that is not a valid answer')\n+\n+\n+@click.command()\n+@click.option('--url', prompt='Enter URL to generate robots.txt',\n+              help='Enter URL to be crawled (must be base path to website)')\n+def url_input(url):\n+    if url.isspace():\n+        click.echo('Enter a URL!')\n+        url_input()\n+    # elif not validate_url(url):\n+    #     click.echo('Invalid URL, please enter a valid URL')\n+    #     url_input()\n+    else:\n+        crawler = Crawler(url)\n+        urls = crawler.get_endpoints_from_url()\n+        post_process(urls, user_agent_list)\n+        path_input()\n+\n+\n+@click.command()\n+@click.option('--path',\n+              prompt='Enter path to generate file(default current dir)',\n+              help='Enter path to be crawled')\n+def path_input(path):\n+    if path.isspace():\n+        robottxt.generate('.')\n+    else:\n+        robottxt.generate(path)\n+\n+\n+# asks user for user agent",
        "path": "robotstxtpy/crawler_cli.py",
        "position": null,
        "original_position": 70,
        "commit_id": "4ccf93bbcbcf2af100a258a075b346847e66ca00",
        "original_commit_id": "fd7b9ffc07d52f8c02d3588a9a65a7c1c8a47e06",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove these comments",
        "created_at": "2020-01-12T19:09:30Z",
        "updated_at": "2020-01-12T19:13:02Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/33#discussion_r365603938",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/33",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365603938"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/33#discussion_r365603938"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/33"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365608479",
        "pull_request_review_id": 341597074,
        "id": 365608479,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwODQ3OQ==",
        "diff_hunk": "@@ -10,3 +10,12 @@ We use `pre-commit` to run linters and formatters before each commit. To setup `\n $ pip install pre-commit\n $ pre-commit install\n ```\n+\n+### Install CLIs\n+\n+To interact with the library functions directly, there are CLI's for this.\n+```bash\n+pip install -e",
        "path": "README.md",
        "position": null,
        "original_position": 9,
        "commit_id": "036ae3ba6f6a3bfeda94014232561c7fb81aa481",
        "original_commit_id": "6c7359ce75bde314b085cf25c33d01deea641669",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "That's should be `pip install -e .` (Don't forget the path `.` \ud83d\ude04 )",
        "created_at": "2020-01-12T20:29:34Z",
        "updated_at": "2020-01-12T20:36:09Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/35#discussion_r365608479",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/35",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365608479"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/35#discussion_r365608479"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/35"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365604038",
        "pull_request_review_id": 341593226,
        "id": 365604038,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwNDAzOA==",
        "diff_hunk": "@@ -0,0 +1,77 @@\n+#!/usr/bin/env python3\n+\n+import re\n+from argparse import ArgumentParser\n+# Check syntax\n+\n+def readText(filename):",
        "path": "robotstxtpy/syntax.py",
        "position": null,
        "original_position": 7,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "f55a19fd66339bd37cb5ccd14874492aca975e80",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "There is no need to read the text file as we could simply take an instance of `RobotsTxt` as the input.",
        "created_at": "2020-01-12T19:11:04Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365604038",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365604038"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365604038"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365604088",
        "pull_request_review_id": 341593226,
        "id": 365604088,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwNDA4OA==",
        "diff_hunk": "@@ -0,0 +1,77 @@\n+#!/usr/bin/env python3\n+\n+import re\n+from argparse import ArgumentParser\n+# Check syntax\n+\n+def readText(filename):\n+    with open(filename, 'r') as f:\n+        line = f.readline()\n+        cnt = 1\n+        lines = []\n+        while line:\n+            lines.append(line.strip())\n+            line = f.readline()\n+            cnt += 1\n+    return lines\n+\n+def checkLine(lines):\n+    s = [\"User-Agent: \", \"Crawl-delay: \", \"Allow: \", \"Disallow: \"]\n+    allowedChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 -._~:/?#[]@!$&'()*+,;=\"\n+    # specialChars = allowedChars.split('-')[1]\n+    # validCombos = ['*.', '-*', '*?']\n+    # print(len(lines))\n+    for count, l in enumerate(lines):\n+        # print(count)",
        "path": "robotstxtpy/syntax.py",
        "position": null,
        "original_position": 25,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "f55a19fd66339bd37cb5ccd14874492aca975e80",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Please remove those comments as they are only used for debugging.",
        "created_at": "2020-01-12T19:12:02Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365604088",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365604088"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365604088"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609778",
        "pull_request_review_id": 341598228,
        "id": 365609778,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTc3OA==",
        "diff_hunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env python3\n+from robotstxtpy import RobotsTxt\n+import re\n+# Check syntax\n+\n+def check(robottxt):\n+    s = [\"Allow\", \"Disallow\"]\n+    allowedChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 -._~:/?#[]@!$&'()*+,;=\"\n+    user_agent = robottxt.user_agents()\n+    print(user_agent)",
        "path": "robotstxtpy/syntax.py",
        "position": null,
        "original_position": 10,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove the `print` statement",
        "created_at": "2020-01-12T20:51:18Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609778",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609778"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609778"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609825",
        "pull_request_review_id": 341598228,
        "id": 365609825,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTgyNQ==",
        "diff_hunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env python3\n+from robotstxtpy import RobotsTxt\n+import re\n+# Check syntax\n+\n+def check(robottxt):\n+    s = [\"Allow\", \"Disallow\"]\n+    allowedChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 -._~:/?#[]@!$&'()*+,;=\"\n+    user_agent = robottxt.user_agents()\n+    print(user_agent)\n+    for agent in user_agent:\n+        print(agent)",
        "path": "robotstxtpy/syntax.py",
        "position": null,
        "original_position": 12,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Ditto",
        "created_at": "2020-01-12T20:51:46Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609825",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609825"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609825"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609833",
        "pull_request_review_id": 341598228,
        "id": 365609833,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTgzMw==",
        "diff_hunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env python3\n+from robotstxtpy import RobotsTxt\n+import re\n+# Check syntax\n+\n+def check(robottxt):\n+    s = [\"Allow\", \"Disallow\"]\n+    allowedChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 -._~:/?#[]@!$&'()*+,;=\"\n+    user_agent = robottxt.user_agents()\n+    print(user_agent)\n+    for agent in user_agent:\n+        print(agent)\n+        cntnt = robottxt.rules(agent)\n+        print(\"cntnt\", cntnt)",
        "path": "robotstxtpy/syntax.py",
        "position": null,
        "original_position": 14,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Ditto",
        "created_at": "2020-01-12T20:51:52Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609833",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609833"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609833"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609849",
        "pull_request_review_id": 341598228,
        "id": 365609849,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTg0OQ==",
        "diff_hunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env python3\n+from robotstxtpy import RobotsTxt\n+import re\n+# Check syntax\n+\n+def check(robottxt):\n+    s = [\"Allow\", \"Disallow\"]\n+    allowedChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 -._~:/?#[]@!$&'()*+,;=\"\n+    user_agent = robottxt.user_agents()\n+    print(user_agent)\n+    for agent in user_agent:\n+        print(agent)\n+        cntnt = robottxt.rules(agent)\n+        print(\"cntnt\", cntnt)\n+        for tup in cntnt:\n+            print(\"in cntnt\", tup)\n+            permission, endpoint = tup[0], tup[1]\n+            print(permission, endpoint)\n+            if permission not in s:\n+                return 0\n+            elif set(allowedChars) <= set(endpoint):\n+                return 0\n+            elif re.search(r\"([+._~:?#[]@!$&'*,;()=-])\\1\", endpoint):\n+                return 0\n+            else:\n+                return 1\n+        \n+def main():\n+    state = 1\n+    rbtxt = RobotsTxt()    \n+    state = check(rbtxt)\n+    if state == 0:\n+        print(\"Syntax error. Please check syntax.\")\n+    else:\n+        print(\"Error free!\")\n+    return\n+\n+if __name__ == \"__main__\":",
        "path": "robotstxtpy/syntax.py",
        "position": null,
        "original_position": 38,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove this",
        "created_at": "2020-01-12T20:52:07Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609849",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609849"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609849"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609872",
        "pull_request_review_id": 341598228,
        "id": 365609872,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTg3Mg==",
        "diff_hunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env python3\n+from robotstxtpy import RobotsTxt\n+import re\n+# Check syntax\n+\n+def check(robottxt):\n+    s = [\"Allow\", \"Disallow\"]\n+    allowedChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 -._~:/?#[]@!$&'()*+,;=\"\n+    user_agent = robottxt.user_agents()\n+    print(user_agent)\n+    for agent in user_agent:\n+        print(agent)\n+        cntnt = robottxt.rules(agent)\n+        print(\"cntnt\", cntnt)\n+        for tup in cntnt:\n+            print(\"in cntnt\", tup)\n+            permission, endpoint = tup[0], tup[1]\n+            print(permission, endpoint)\n+            if permission not in s:\n+                return 0\n+            elif set(allowedChars) <= set(endpoint):\n+                return 0\n+            elif re.search(r\"([+._~:?#[]@!$&'*,;()=-])\\1\", endpoint):\n+                return 0\n+            else:\n+                return 1\n+        \n+def main():",
        "path": "robotstxtpy/syntax.py",
        "position": 23,
        "original_position": 28,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove this",
        "created_at": "2020-01-12T20:52:35Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609872",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609872"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609872"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609905",
        "pull_request_review_id": 341598228,
        "id": 365609905,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTkwNQ==",
        "diff_hunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env python3\n+from robotstxtpy import RobotsTxt\n+import re\n+# Check syntax\n+\n+def check(robottxt):",
        "path": "robotstxtpy/syntax.py",
        "position": 6,
        "original_position": 6,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Please return a boolean to indicate if the `check` is successful",
        "created_at": "2020-01-12T20:53:13Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609905",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609905"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609905"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609952",
        "pull_request_review_id": 341598228,
        "id": 365609952,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYwOTk1Mg==",
        "diff_hunk": "@@ -0,0 +1,13 @@\n+#!/usr/bin/env python3\n+from robotstxtpy.syntax import check\n+from robotstxtpy import RobotsTxt\n+\n+def test_check():\n+    rbtxt = RobotsTxt()\n+    rbtxt.add_user_agent(\"Chrome\")\n+    rbtxt.add_endpoint(\"Chrome\", \"Allow\", \"test\")\n+    assert check(rbtxt) == 1\n+\n+if __name__ == \"__main__\":",
        "path": "tests/robotstxtpy/test_syntax.py",
        "position": 11,
        "original_position": 11,
        "commit_id": "50a1b7b3545afaea3545e580f6792323a80608c7",
        "original_commit_id": "ec1f7b848adc0ae73a4ef5a961bf43b0478737cb",
        "user": {
            "login": "algobot76",
            "id": 20016835,
            "node_id": "MDQ6VXNlcjIwMDE2ODM1",
            "avatar_url": "https://avatars2.githubusercontent.com/u/20016835?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/algobot76",
            "html_url": "https://github.com/algobot76",
            "followers_url": "https://api.github.com/users/algobot76/followers",
            "following_url": "https://api.github.com/users/algobot76/following{/other_user}",
            "gists_url": "https://api.github.com/users/algobot76/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/algobot76/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/algobot76/subscriptions",
            "organizations_url": "https://api.github.com/users/algobot76/orgs",
            "repos_url": "https://api.github.com/users/algobot76/repos",
            "events_url": "https://api.github.com/users/algobot76/events{/privacy}",
            "received_events_url": "https://api.github.com/users/algobot76/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove this. Use `pytest` in your terminal instead.",
        "created_at": "2020-01-12T20:53:56Z",
        "updated_at": "2020-01-12T21:04:25Z",
        "html_url": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609952",
        "pull_request_url": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30",
        "author_association": "OWNER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/comments/365609952"
            },
            "html": {
                "href": "https://github.com/algobot76/robotstxtpy/pull/30#discussion_r365609952"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/algobot76/robotstxtpy/pulls/30"
            }
        }
    }
]