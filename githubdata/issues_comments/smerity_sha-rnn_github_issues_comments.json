[
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/559486163",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/3#issuecomment-559486163",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3",
        "id": 559486163,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU1OTQ4NjE2Mw==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-11-28T13:02:03Z",
        "updated_at": "2019-11-28T13:06:38Z",
        "author_association": "OWNER",
        "body": "Thanks for running this from scratch and I'm glad you're already almost to replicating it! I'll be able to get help you get the rest of the way as the mistakes are issues in the commands I've included in the README :)\r\n\r\nThe faster training time is as \"the single headed SHA-LSTM is that each epoch took almost exactly 1800 \u00b1 1 seconds (30 minutes) compared to the 4 headed SHA-LSTM which took 4020 seconds (67 minutes)\" and the code currently runs the single-headed SHA-LSTM. That maps fairly well to your 5493 batches * 0.27 seconds per batch ~= 25 minutes per epoch.\r\n\r\nIf you'd like to use the full 4 headed SHA-LSTM (which requires a batch size of 8 on the Titan V but gets a slightly better result as noted in the paper but twice as slow - your V100 may be able to get a larger batch though!) you can set `use_attn` to True for all layers [on this line](https://github.com/Smerity/sha-rnn/blob/0fd656eb25518581c5fe09d53d6a52df6473744a/model.py#L255). Sorry for not making that a command line flag - I was setting options internally manually.\r\n\r\nThe commands supplied were originally in reference to the 4 layer SHA-LSTM where each layer contains an attention mechanism, not the single headed SHA-LSTM. The batch size 16 model requires a few extra epochs as there are less training steps per epoch compared to the batch size 8 model. As such the limit of 14 epochs for training is incorrect and was in reference to the full 4 layer SHA-LSTM (i.e. reproducing Figure 3 of the paper) which only used 19 epochs total - 16 using `lr=2e-3` and 3 using `lr=1e-3`.\r\n\r\n---\r\n\r\nFor reproducing with the single headed SHA-LSTM, training with the first command (`lr=2e-3`) should continue until the validation perplexity stops improving. You can leave the model running longer too as it'll only save models with better validation perplexity than previously discovered.\r\n\r\nTraining with the second command (`lr=1e-3`) generally only takes two or three epochs until validation perplexity stops improving. The majority of the gain is usually in the first epoch.\r\n\r\n---\r\n\r\nIf you still have the log you can run `grep \"valid bpc\"` and we can verify whether the model was plateauing at epoch 14 using the learning rate of 2e-3. As an example below is a log for the single headed SHA-LSTM showing that there was still a bit of improvement to be made at epoch 14. It was my error in not updating the commands to have higher epoch counts when transitioning models.\r\n\r\n```\r\n| end of epoch   1 | time: 1813.52s | valid loss  0.92 | valid ppl     2.52 | valid bpc    1.334\r\n| end of epoch   2 | time: 1811.97s | valid loss  0.87 | valid ppl     2.38 | valid bpc    1.252\r\n| end of epoch   3 | time: 1815.17s | valid loss  0.84 | valid ppl     2.32 | valid bpc    1.214\r\n| end of epoch   4 | time: 1813.15s | valid loss  0.82 | valid ppl     2.28 | valid bpc    1.189\r\n| end of epoch   5 | time: 1812.83s | valid loss  0.81 | valid ppl     2.25 | valid bpc    1.171\r\n| end of epoch   6 | time: 1815.57s | valid loss  0.80 | valid ppl     2.23 | valid bpc    1.160\r\n| end of epoch   7 | time: 1809.98s | valid loss  0.80 | valid ppl     2.22 | valid bpc    1.149\r\n| end of epoch   8 | time: 1806.74s | valid loss  0.79 | valid ppl     2.21 | valid bpc    1.142\r\n| end of epoch   9 | time: 1814.90s | valid loss  0.79 | valid ppl     2.20 | valid bpc    1.138\r\n| end of epoch  10 | time: 1805.94s | valid loss  0.79 | valid ppl     2.19 | valid bpc    1.134\r\n| end of epoch  11 | time: 1803.10s | valid loss  0.78 | valid ppl     2.19 | valid bpc    1.129\r\n| end of epoch  12 | time: 1800.77s | valid loss  0.78 | valid ppl     2.18 | valid bpc    1.125\r\n| end of epoch  13 | time: 1801.50s | valid loss  0.78 | valid ppl     2.18 | valid bpc    1.123\r\n| end of epoch  14 | time: 1797.74s | valid loss  0.78 | valid ppl     2.17 | valid bpc    1.120\r\n| end of epoch  15 | time: 1799.41s | valid loss  0.78 | valid ppl     2.18 | valid bpc    1.121\r\n| end of epoch  16 | time: 1796.43s | valid loss  0.77 | valid ppl     2.17 | valid bpc    1.117\r\n| end of epoch  17 | time: 1797.27s | valid loss  0.77 | valid ppl     2.17 | valid bpc    1.118\r\n| end of epoch  18 | time: 1798.39s | valid loss  0.77 | valid ppl     2.17 | valid bpc    1.116\r\n| end of epoch  19 | time: 1799.21s | valid loss  0.77 | valid ppl     2.17 | valid bpc    1.114\r\n| end of epoch  20 | time: 1799.06s | valid loss  0.77 | valid ppl     2.17 | valid bpc    1.115\r\n| end of epoch  21 | time: 1799.13s | valid loss  0.77 | valid ppl     2.16 | valid bpc    1.114\r\n| end of epoch  22 | time: 1799.01s | valid loss  0.77 | valid ppl     2.16 | valid bpc    1.113\r\n| end of epoch  23 | time: 1803.80s | valid loss  0.77 | valid ppl     2.16 | valid bpc    1.112\r\n| end of epoch  24 | time: 1804.56s | valid loss  0.77 | valid ppl     2.16 | valid bpc    1.112\r\n| end of epoch  25 | time: 1806.76s | valid loss  0.77 | valid ppl     2.16 | valid bpc    1.112\r\n| end of epoch  26 | time: 1807.95s | valid loss  0.77 | valid ppl     2.17 | valid bpc    1.115\r\n| end of epoch  27 | time: 1805.45s | valid loss  0.77 | valid ppl     2.16 | valid bpc    1.112\r\n```\r\n\r\nThat set of 27 epochs is about 13.5 hours of compute and gets to approximately your number. I killed training at that stage as the validation perplexity stopped improving. Then I resumed with `lr=1e-3` and also the same batch size (16). Unfortunately the batch size 8 that you did was as I hadn't updated the README properly sorry -_-\r\n\r\n```\r\n| end of epoch   1 | time: 1799.11s | valid loss  0.76 | valid ppl     2.14 | valid bpc    1.100\r\n| end of epoch   2 | time: 1810.37s | valid loss  0.76 | valid ppl     2.14 | valid bpc    1.100\r\n| end of epoch   3 | time: 1814.39s | valid loss  0.76 | valid ppl     2.14 | valid bpc    1.100\r\n| end of epoch   4 | time: 1801.80s | valid loss  0.76 | valid ppl     2.15 | valid bpc    1.102\r\n| end of epoch   5 | time: 1800.42s | valid loss  0.76 | valid ppl     2.14 | valid bpc    1.100\r\n```\r\n\r\nAs noted all the benefit in the case above is on the first epoch. Sometimes the drop is over a few epochs. There are likely better ways to decay the learning rate but I hadn't explored them.\r\n\r\nThe above model resulted in a test bpc of 1.078.\r\n\r\n---\r\n\r\nIf you have spare compute and want to try it again, do so and get back to me. Otherwise I'll be repeating the experiment myself overnight (... it's 5am but I'll pretend it's overnight lol ...) and report back. Do also note if you have a preference for the faster model or the slightly better model but slower / heavier model. I might have gone the wrong direction by setting the slightly faster one as the default for the codebase.\r\n\r\nThanks for your experiment! ^_^"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/559491726",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/3#issuecomment-559491726",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3",
        "id": 559491726,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU1OTQ5MTcyNg==",
        "user": {
            "login": "pgmmpk",
            "id": 569458,
            "node_id": "MDQ6VXNlcjU2OTQ1OA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/569458?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pgmmpk",
            "html_url": "https://github.com/pgmmpk",
            "followers_url": "https://api.github.com/users/pgmmpk/followers",
            "following_url": "https://api.github.com/users/pgmmpk/following{/other_user}",
            "gists_url": "https://api.github.com/users/pgmmpk/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pgmmpk/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pgmmpk/subscriptions",
            "organizations_url": "https://api.github.com/users/pgmmpk/orgs",
            "repos_url": "https://api.github.com/users/pgmmpk/repos",
            "events_url": "https://api.github.com/users/pgmmpk/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pgmmpk/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-11-28T13:18:47Z",
        "updated_at": "2019-11-28T13:18:47Z",
        "author_association": "NONE",
        "body": "Thanks for the quick and detailed reply. I was foolish enough to run this in terminal, so the logs are mostly lost to the limited terminal scrolling window :(. As I watched the first run, I am pretty sure it plateaued at the last few epochs. Validation bpcs were a bit lagging, but fairly close to those plotted in the article. \r\n\r\nYes, makes sense to do the batch of 16 on the second run and just few epochs. If I have time I'll repeat the experiment and report here.\r\n\r\nLove the speed and high GPU utilization. Thank you for publishing this. Not everyone has 1K of TPUs, this thing gives us poor guys some hope ;)"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/559557374",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/3#issuecomment-559557374",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3",
        "id": 559557374,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU1OTU1NzM3NA==",
        "user": {
            "login": "mkroutikov",
            "id": 14280777,
            "node_id": "MDQ6VXNlcjE0MjgwNzc3",
            "avatar_url": "https://avatars2.githubusercontent.com/u/14280777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mkroutikov",
            "html_url": "https://github.com/mkroutikov",
            "followers_url": "https://api.github.com/users/mkroutikov/followers",
            "following_url": "https://api.github.com/users/mkroutikov/following{/other_user}",
            "gists_url": "https://api.github.com/users/mkroutikov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mkroutikov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mkroutikov/subscriptions",
            "organizations_url": "https://api.github.com/users/mkroutikov/orgs",
            "repos_url": "https://api.github.com/users/mkroutikov/repos",
            "events_url": "https://api.github.com/users/mkroutikov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mkroutikov/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-11-28T16:44:23Z",
        "updated_at": "2019-11-28T16:44:23Z",
        "author_association": "NONE",
        "body": "```\r\n| epoch   4 |  5580/ 5493 batches | lr 0.00100 | ms/batch 265.68 | loss  0.70 | ppl     2.02 | bpc    1.012\r\n| epoch   4 |  5590/ 5493 batches | lr 0.00100 | ms/batch 253.69 | loss  0.68 | ppl     1.96 | bpc    0.974\r\n| epoch   4 |  5600/ 5493 batches | lr 0.00100 | ms/batch 268.03 | loss  0.71 | ppl     2.04 | bpc    1.028\r\n| epoch   4 |  5610/ 5493 batches | lr 0.00100 | ms/batch 250.21 | loss  0.70 | ppl     2.02 | bpc    1.011\r\n| epoch   4 |  5620/ 5493 batches | lr 0.00100 | ms/batch 258.41 | loss  0.69 | ppl     1.99 | bpc    0.989\r\n| epoch   4 |  5630/ 5493 batches | lr 0.00100 | ms/batch 265.86 | loss  0.70 | ppl     2.02 | bpc    1.013\r\n-----------------------------------------------------------------------------------------\r\n| end of epoch   5 | time: 1481.99s | valid loss  0.76 | valid ppl     2.14 | valid bpc    1.099\r\n-----------------------------------------------------------------------------------------\r\nModel total parameters: 53790932\r\n=========================================================================================\r\n| End of training | test loss  0.75 | test ppl     2.11 | test bpc    1.077\r\n=========================================================================================\r\n```\r\nI did this:\r\n1. Loaded the model trained overnight (see the link above). Trained it with `lr=2e-3` and batch size `16` for 2 epochs (to shake it up a bit)\r\n2. Fine-tuned using `lr=1e-3` and batch size `16` for 5 epochs (it took 2 epochs to get to the best validation, rest was waste). \r\n\r\nThe latest trained model can be downloaded from [here](https://s3.amazonaws.com/temp.innodatalabs.com/ENWIK8-finetuned.pt).\r\n\r\nSummary: all works as advertised!"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/562052609",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/2#issuecomment-562052609",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/2",
        "id": 562052609,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2MjA1MjYwOQ==",
        "user": {
            "login": "PiotrCzapla",
            "id": 340180,
            "node_id": "MDQ6VXNlcjM0MDE4MA==",
            "avatar_url": "https://avatars1.githubusercontent.com/u/340180?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PiotrCzapla",
            "html_url": "https://github.com/PiotrCzapla",
            "followers_url": "https://api.github.com/users/PiotrCzapla/followers",
            "following_url": "https://api.github.com/users/PiotrCzapla/following{/other_user}",
            "gists_url": "https://api.github.com/users/PiotrCzapla/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PiotrCzapla/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PiotrCzapla/subscriptions",
            "organizations_url": "https://api.github.com/users/PiotrCzapla/orgs",
            "repos_url": "https://api.github.com/users/PiotrCzapla/repos",
            "events_url": "https://api.github.com/users/PiotrCzapla/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PiotrCzapla/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-05T09:46:56Z",
        "updated_at": "2019-12-05T09:49:52Z",
        "author_association": "NONE",
        "body": "Steven It would be useful to have an LSTM based network shown on wikitext103 benchmark on sotabench. If you don't have the pre-trained weights anymore I can run the training but it would be good to host it here in this repo.  \r\nhttps://sotabench.com/benchmarks/language-modelling-on-wikitext-103\r\n"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/563500329",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/4#issuecomment-563500329",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/4",
        "id": 563500329,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2MzUwMDMyOQ==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T00:16:41Z",
        "updated_at": "2019-12-10T00:16:41Z",
        "author_association": "OWNER",
        "body": "Good suggestion! Whilst the model itself is relatively simple and should be easily re-implemented in Tensorflow I haven't used TF for some time however and don't know what the best practices would be. I tend to think this would do well as a separate repository too."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/563503705",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/7#issuecomment-563503705",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/7",
        "id": 563503705,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2MzUwMzcwNQ==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T00:30:30Z",
        "updated_at": "2019-12-10T00:30:30Z",
        "author_association": "OWNER",
        "body": "Thanks for running this on German @stefan-it! I haven't done experiments on different languages yet and it's great to see it at least hold!\r\n\r\nUnfortunately I have run into similar issues in the past re: NaNs at around 15 or 20 epochs on the `enwik8` data which works out to around 2 epochs on your larger German dataset.\r\n\r\nI still haven't tracked down exactly what it might be but I do know the random seed can impact it. My guess is that there might be an issue with dropout over the attention window or similar.\r\n\r\nI'll be making a new cleaner codebase and ensuring that issues like this don't occur will be a top priority. As a temporary fix if you're curious to continue investigating you could save the model once every N iterations if the loss hasn't NaN'ed out and restart as you've done with a different random seed. That's admittedly not a great solution however.\r\n\r\nI'm glad you're enjoying the generated text! I ran it through Google Translate and it at least produces something I can read lol. I'll note that for this model the more context you can seed it with the better!"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/563507117",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/8#issuecomment-563507117",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/8",
        "id": 563507117,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2MzUwNzExNw==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T00:43:47Z",
        "updated_at": "2019-12-10T00:43:47Z",
        "author_association": "OWNER",
        "body": "The main issue is if a token occurs in validation or test without appearing in training then it's bad news for the model. The weights will be uninitialized at best.\r\n\r\nUsing wordpieces would likely be the best solution. You could also do what the Penn Treebank (PTB) did and add each of the words found in validation/test at the start or end of the training file. Not an optimal solution but it is a solution at least. You could also add an unknown token (`<unk>`) to the dataset as well."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/563508267",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-563508267",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 563508267,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2MzUwODI2Nw==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T00:48:29Z",
        "updated_at": "2019-12-10T00:48:29Z",
        "author_association": "OWNER",
        "body": "At a guess the likely issue is the vocabulary size of your dataset. What's the vocabulary size you have for your 3.3GB dataset? The dataset isn't actually kept on the GPU device's memory so shouldn't impact the model size.\r\n\r\nThe solutions would include an adaptive softmax, which this codebase used to have but which I removed, or to reduce the vocabulary size through wordpieces or similar.\r\n\r\nIf you have a large vocabulary then the GPU memory will balloon quite rapidly as it's required for the softmax output of each and every timestep."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/563565737",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-563565737",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 563565737,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2MzU2NTczNw==",
        "user": {
            "login": "jprobichaud",
            "id": 9254379,
            "node_id": "MDQ6VXNlcjkyNTQzNzk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/9254379?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jprobichaud",
            "html_url": "https://github.com/jprobichaud",
            "followers_url": "https://api.github.com/users/jprobichaud/followers",
            "following_url": "https://api.github.com/users/jprobichaud/following{/other_user}",
            "gists_url": "https://api.github.com/users/jprobichaud/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jprobichaud/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jprobichaud/subscriptions",
            "organizations_url": "https://api.github.com/users/jprobichaud/orgs",
            "repos_url": "https://api.github.com/users/jprobichaud/repos",
            "events_url": "https://api.github.com/users/jprobichaud/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jprobichaud/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T01:55:56Z",
        "updated_at": "2019-12-10T01:55:56Z",
        "author_association": "NONE",
        "body": "It's a char-based lm, and the data is lowercased, so aside the 26 letters,\nsome apostrophes and dashes plus some monetary symbols, there is nothing\nelse.  The vocab size is less than 100.\n\nHow can i diagnose this issue?\n\nOn Mon., Dec. 9, 2019, 7:48 p.m. Stephen Merity, <notifications@github.com>\nwrote:\n\n> At a guess the likely issue is the vocabulary size of your dataset. What's\n> the vocabulary size you have for your 3.3GB dataset? The dataset isn't\n> actually kept on the GPU device's memory so shouldn't impact the model size.\n>\n> The solutions would include an adaptive softmax, which this codebase used\n> to have but which I removed, or to reduce the vocabulary size through\n> wordpieces or similar.\n>\n> If you have a large vocabulary then the GPU memory will balloon quite\n> rapidly as it's required for the softmax output of each and every timestep.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Smerity/sha-rnn/issues/9?email_source=notifications&email_token=ACGTL2Z5WIG2YDAOKDJDUSLQX3RN3A5CNFSM4JXBSM52YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGLHIKY#issuecomment-563508267>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGTL27DVIMDPKSTAKTBAPTQX3RN3ANCNFSM4JXBSM5Q>\n> .\n>\n"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/564184799",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-564184799",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 564184799,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2NDE4NDc5OQ==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T19:07:56Z",
        "updated_at": "2019-12-10T19:07:56Z",
        "author_association": "OWNER",
        "body": "That's quite odd. Are you able to replicate the initial results on `enwik8`? I would try doing that first. My GPU only had ~12GB of RAM so there's no reason you shouldn't be able to do this as far as I'm aware assuming your data is character level. If you can replicate then try a 100MB chunk of your dataset and if that still works then potentially I do have a line of code that unexpectedly puts the dataset in GPU memory. If that's the case it's an easy fix of finding that line (like a `.cuda()`), removing that from the massive dataset, and putting a `.cuda()` when the snippets of data are loaded for training."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/564265550",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-564265550",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 564265550,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2NDI2NTU1MA==",
        "user": {
            "login": "jprobichaud",
            "id": 9254379,
            "node_id": "MDQ6VXNlcjkyNTQzNzk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/9254379?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jprobichaud",
            "html_url": "https://github.com/jprobichaud",
            "followers_url": "https://api.github.com/users/jprobichaud/followers",
            "following_url": "https://api.github.com/users/jprobichaud/following{/other_user}",
            "gists_url": "https://api.github.com/users/jprobichaud/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jprobichaud/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jprobichaud/subscriptions",
            "organizations_url": "https://api.github.com/users/jprobichaud/orgs",
            "repos_url": "https://api.github.com/users/jprobichaud/repos",
            "events_url": "https://api.github.com/users/jprobichaud/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jprobichaud/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T21:25:20Z",
        "updated_at": "2019-12-10T21:25:20Z",
        "author_association": "NONE",
        "body": "I was able to reproduce the enwik8 results without problem (not the exact\nBPC published, but very close)\n\nI will try with a smaller sample of my dataset and see.  If need be, I'll\ngo and see if there is a .cuda() put in the wrong place.\n\nI had added some print statements in the data loading method, here are the\nnumbers I'm getting (for the entire dataset):\ntrain.txt, 1555434404 tokens\nvalid.txt, 1978645700 tokens\ntest.txt, 2375699684 tokens\n\n\n\n\nOn Tue, Dec 10, 2019 at 2:07 PM Stephen Merity <notifications@github.com>\nwrote:\n\n> That's quite odd. Are you able to replicate the initial results on enwik8?\n> I would try doing that first. My GPU only had ~12GB of RAM so there's no\n> reason you shouldn't be able to do this as far as I'm aware assuming your\n> data is character level. If you can replicate then try a 100MB chunk of\n> your dataset and if that still works then potentially I do have a line of\n> code that unexpectedly puts the dataset in GPU memory. If that's the case\n> it's an easy fix of finding that line (like a .cuda()), removing that\n> from the massive dataset, and putting a .cuda() when the snippets of data\n> are loaded for training.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Smerity/sha-rnn/issues/9?email_source=notifications&email_token=ACGTL2YX4AEGWS4DWHNMCT3QX7SIZA5CNFSM4JXBSM52YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGQMNXY#issuecomment-564184799>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACGTL26UULVJQNW6EUSAOBLQX7SIZANCNFSM4JXBSM5Q>\n> .\n>\n"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/564271307",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-564271307",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 564271307,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2NDI3MTMwNw==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T21:40:39Z",
        "updated_at": "2019-12-10T21:40:39Z",
        "author_association": "OWNER",
        "body": "Ah, I was wrong. The dataset is loaded into memory - it was a previous version of the codebase I optimized that for sorry.\r\n\r\nThe fix is to take out the [dataset transfer to GPU in batchify](https://github.com/Smerity/sha-rnn/blob/0fd656eb25518581c5fe09d53d6a52df6473744a/utils.py#L34) and add it to `data` and `target` in [get_batch](https://github.com/Smerity/sha-rnn/blob/0fd656eb25518581c5fe09d53d6a52df6473744a/utils.py#L42).\r\n\r\nThis may slow the training down a little, I'm not certain, as small batches of data will be shuffled back and forth between CPU and GPU, but it will allow you to train without having the dataset in GPU RAM. You'll obviously need to store it in CPU RAM however."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/564277533",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-564277533",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 564277533,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2NDI3NzUzMw==",
        "user": {
            "login": "jprobichaud",
            "id": 9254379,
            "node_id": "MDQ6VXNlcjkyNTQzNzk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/9254379?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jprobichaud",
            "html_url": "https://github.com/jprobichaud",
            "followers_url": "https://api.github.com/users/jprobichaud/followers",
            "following_url": "https://api.github.com/users/jprobichaud/following{/other_user}",
            "gists_url": "https://api.github.com/users/jprobichaud/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jprobichaud/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jprobichaud/subscriptions",
            "organizations_url": "https://api.github.com/users/jprobichaud/orgs",
            "repos_url": "https://api.github.com/users/jprobichaud/repos",
            "events_url": "https://api.github.com/users/jprobichaud/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jprobichaud/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T21:57:31Z",
        "updated_at": "2019-12-10T21:57:31Z",
        "author_association": "NONE",
        "body": "Wonderful, thanks, that seems to do the trick!\r\n\r\nWith a smaller dataset and without the fix, I'm getting the following \"throughput\" \r\n```\r\nDec 10 16:51:40 | epoch   0 |    10/ 1162 batches | lr 0.00003 | ms/batch 689.93 | loss  4.57 | ppl    96.23 | bpc    6.588\r\nDec 10 16:51:47 | epoch   0 |    20/ 1162 batches | lr 0.00005 | ms/batch 651.88 | loss  3.65 | ppl    38.31 | bpc    5.260\r\nDec 10 16:51:53 | epoch   0 |    30/ 1162 batches | lr 0.00008 | ms/batch 653.78 | loss  3.12 | ppl    22.68 | bpc    4.503\r\nDec 10 16:52:00 | epoch   0 |    40/ 1162 batches | lr 0.00010 | ms/batch 657.68 | loss  3.01 | ppl    20.19 | bpc    4.336\r\nDec 10 16:52:07 | epoch   0 |    50/ 1162 batches | lr 0.00013 | ms/batch 661.37 | loss  2.99 | ppl    19.88 | bpc    4.313\r\nDec 10 16:52:13 | epoch   0 |    60/ 1162 batches | lr 0.00015 | ms/batch 634.67 | loss  3.00 | ppl    20.03 | bpc    4.324\r\nDec 10 16:52:20 | epoch   0 |    70/ 1162 batches | lr 0.00018 | ms/batch 662.47 | loss  2.97 | ppl    19.54 | bpc    4.289\r\nDec 10 16:52:26 | epoch   0 |    80/ 1162 batches | lr 0.00020 | ms/batch 671.82 | loss  2.88 | ppl    17.74 | bpc    4.149\r\nDec 10 16:52:33 | epoch   0 |    90/ 1162 batches | lr 0.00023 | ms/batch 670.94 | loss  2.76 | ppl    15.81 | bpc    3.983\r\nDec 10 16:52:40 | epoch   0 |   100/ 1162 batches | lr 0.00025 | ms/batch 673.17 | loss  2.66 | ppl    14.26 | bpc    3.834\r\nDec 10 16:52:46 | epoch   0 |   110/ 1162 batches | lr 0.00028 | ms/batch 672.23 | loss  2.58 | ppl    13.18 | bpc    3.720\r\nDec 10 16:52:53 | epoch   0 |   120/ 1162 batches | lr 0.00030 | ms/batch 674.66 | loss  2.47 | ppl    11.80 | bpc    3.560\r\nDec 10 16:53:00 | epoch   0 |   130/ 1162 batches | lr 0.00033 | ms/batch 674.38 | loss  2.37 | ppl    10.70 | bpc    3.419\r\nDec 10 16:53:07 | epoch   0 |   140/ 1162 batches | lr 0.00035 | ms/batch 676.15 | loss  2.32 | ppl    10.15 | bpc    3.343\r\nDec 10 16:53:14 | epoch   0 |   150/ 1162 batches | lr 0.00038 | ms/batch 709.25 | loss  2.24 | ppl     9.42 | bpc    3.236\r\n```\r\n\r\nso 1.6 batches per sec.\r\n\r\n\r\nWith the larger dataset and the fix you suggested:\r\n```\r\nDec 10 16:50:05 | epoch   0 |    10/94936 batches | lr 0.00003 | ms/batch 1056.88 | loss  4.57 | ppl    96.52 | bpc    6.593\r\nDec 10 16:50:15 | epoch   0 |    20/94936 batches | lr 0.00005 | ms/batch 938.93 | loss  3.65 | ppl    38.34 | bpc    5.261\r\nDec 10 16:50:21 | epoch   0 |    30/94936 batches | lr 0.00008 | ms/batch 673.42 | loss  3.11 | ppl    22.46 | bpc    4.489\r\nDec 10 16:50:28 | epoch   0 |    40/94936 batches | lr 0.00010 | ms/batch 677.31 | loss  3.01 | ppl    20.21 | bpc    4.337\r\nDec 10 16:50:35 | epoch   0 |    50/94936 batches | lr 0.00013 | ms/batch 683.31 | loss  2.99 | ppl    19.96 | bpc    4.319\r\nDec 10 16:50:42 | epoch   0 |    60/94936 batches | lr 0.00015 | ms/batch 688.04 | loss  3.00 | ppl    20.01 | bpc    4.323\r\nDec 10 16:50:49 | epoch   0 |    70/94936 batches | lr 0.00018 | ms/batch 711.40 | loss  2.99 | ppl    19.79 | bpc    4.307\r\nDec 10 16:50:56 | epoch   0 |    80/94936 batches | lr 0.00020 | ms/batch 713.90 | loss  2.88 | ppl    17.80 | bpc    4.154\r\nDec 10 16:51:03 | epoch   0 |    90/94936 batches | lr 0.00023 | ms/batch 714.09 | loss  2.75 | ppl    15.60 | bpc    3.964\r\nDec 10 16:51:10 | epoch   0 |   100/94936 batches | lr 0.00025 | ms/batch 717.38 | loss  2.67 | ppl    14.45 | bpc    3.853\r\nDec 10 16:51:17 | epoch   0 |   110/94936 batches | lr 0.00028 | ms/batch 713.74 | loss  2.58 | ppl    13.20 | bpc    3.722\r\nDec 10 16:51:25 | epoch   0 |   120/94936 batches | lr 0.00030 | ms/batch 711.81 | loss  2.49 | ppl    12.08 | bpc    3.595\r\nDec 10 16:51:31 | epoch   0 |   130/94936 batches | lr 0.00033 | ms/batch 682.43 | loss  2.43 | ppl    11.33 | bpc    3.502\r\nDec 10 16:51:38 | epoch   0 |   140/94936 batches | lr 0.00035 | ms/batch 672.41 | loss  2.34 | ppl    10.36 | bpc    3.372\r\nDec 10 16:51:45 | epoch   0 |   150/94936 batches | lr 0.00038 | ms/batch 713.46 | loss  2.27 | ppl     9.65 | bpc    3.270\r\n```\r\n\r\nSo about 1.5 batches per sec. \r\n\r\nNot bad.  Both exps use `--emsize 512 --nhid 4096 --nlayers 4 --batch_size 16`\r\n\r\nThe large data set o GPU 0, the \"small data set\" runs on GPU 1 and nvidia-smi reports:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     36533      C   python                                      6063MiB |\r\n|    1     36623      C   python                                      8795MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/564280788",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9#issuecomment-564280788",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "id": 564280788,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU2NDI4MDc4OA==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2019-12-10T22:06:23Z",
        "updated_at": "2019-12-10T22:06:23Z",
        "author_association": "OWNER",
        "body": "I'm so glad! Sorry about the wild goose / bug chase =]\r\n\r\nIt appears that the overhead isn't all that substantial which is reassuring. The technique of loading individual batches to GPU memory was the approach I used for WikiText-103 as RAM was scarce. Various optimizations could be made, such as loading a number of batches at the same time, but that's likely a little over the top. There are big gains to come from all directions as the model really deserves some optimization love.\r\n\r\nFor your experiment I would note that the embedding size of 512 will likely limit your model as that's the size of the LSTM hidden state as well. LSTMs are not as efficient when working with smaller hidden states due to the forget mask recurrence limiting their expressiveness. You should still get reasonable results but it may require some tweaking.\r\n\r\nIf you're interested in telling me more about what dataset / task you're exploring I'd love to hear it, online or offline :)"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/578388207",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/11#issuecomment-578388207",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/11",
        "id": 578388207,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3ODM4ODIwNw==",
        "user": {
            "login": "Smerity",
            "id": 32325,
            "node_id": "MDQ6VXNlcjMyMzI1",
            "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Smerity",
            "html_url": "https://github.com/Smerity",
            "followers_url": "https://api.github.com/users/Smerity/followers",
            "following_url": "https://api.github.com/users/Smerity/following{/other_user}",
            "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions",
            "organizations_url": "https://api.github.com/users/Smerity/orgs",
            "repos_url": "https://api.github.com/users/Smerity/repos",
            "events_url": "https://api.github.com/users/Smerity/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Smerity/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-25T08:34:11Z",
        "updated_at": "2020-01-25T08:34:11Z",
        "author_association": "OWNER",
        "body": "Where are you seeing this? I'm looking at the paper for enwik8 results in\nTable 2. Maybe you were looking at Table 1 which is text8? Nowhere can I\nsee 1.3 bpc and that result would be higher than SotA two or three years\nago.\n\nThanks for keeping an eye out though!\n\nhttps://arxiv.org/abs/1905.07799\n\nOn Sat, Jan 25, 2020 at 12:20 AM djstrong <notifications@github.com> wrote:\n\n> In the readme Adaptive Span Transformer is referred as small, but actually\n> it is normal version. Small version achieves 1.3 BPC (BPB actually).\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Smerity/sha-rnn/issues/11?email_source=notifications&email_token=AAAH4RPFW5CMVW3LCDHZUFLQ7PY47A5CNFSM4KLPUSR2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4IIVQW2Q>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAH4RLHQUA2M2MDN2ZYZ2LQ7PY47ANCNFSM4KLPUSRQ>\n> .\n>\n-- \n\nRegards,\nStephen Merity\n\ne: smerity@smerity.com\nw: smerity.com\nlinkedin: http://au.linkedin.com/in/smerity\n"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/comments/578392218",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/11#issuecomment-578392218",
        "issue_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/11",
        "id": 578392218,
        "node_id": "MDEyOklzc3VlQ29tbWVudDU3ODM5MjIxOA==",
        "user": {
            "login": "djstrong",
            "id": 1849959,
            "node_id": "MDQ6VXNlcjE4NDk5NTk=",
            "avatar_url": "https://avatars1.githubusercontent.com/u/1849959?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/djstrong",
            "html_url": "https://github.com/djstrong",
            "followers_url": "https://api.github.com/users/djstrong/followers",
            "following_url": "https://api.github.com/users/djstrong/following{/other_user}",
            "gists_url": "https://api.github.com/users/djstrong/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/djstrong/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/djstrong/subscriptions",
            "organizations_url": "https://api.github.com/users/djstrong/orgs",
            "repos_url": "https://api.github.com/users/djstrong/repos",
            "events_url": "https://api.github.com/users/djstrong/events{/privacy}",
            "received_events_url": "https://api.github.com/users/djstrong/received_events",
            "type": "User",
            "site_admin": false
        },
        "created_at": "2020-01-25T09:45:08Z",
        "updated_at": "2020-01-25T09:45:44Z",
        "author_association": "NONE",
        "body": "You are right, in the paper they present small and large models.\r\n\r\nHowever, I was looking at their repo (https://github.com/facebookresearch/adaptive-span):\r\n\r\n>Scripts for running experiments in the paper are located in ./experiments/ directory. For example, a smaller 8-layer version of our model can be trained on a single GPU by running:\r\n>\r\n>`bash experiments/enwiki8_small.sh`\r\nIt should reach about 1.3bpc on dev after 150k steps.\r\n>\r\n>...\r\n>\r\n>Experiment | #params | dev (bpc) | test (bpc)\r\n>-- | -- | -- | --\r\n>enwik8 | 38M | 1.04 | 1.02\r\n>enwik8_large | 209M | 1.00 | 0.98\r\n\r\nIn the experiments directory are scripts: `enwik8_small.sh`, `enwik8.sh`, `enwik8_large.sh`. So, \"small\" from the paper is `enwik8.sh`. That confused me."
    }
]