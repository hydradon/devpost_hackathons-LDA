[
    {
        "url": "https://api.github.com/repos/mccm-innovations/UiPath_Document_OCR/issues/1",
        "repository_url": "https://api.github.com/repos/mccm-innovations/UiPath_Document_OCR",
        "labels_url": "https://api.github.com/repos/mccm-innovations/UiPath_Document_OCR/issues/1/labels{/name}",
        "comments_url": "https://api.github.com/repos/mccm-innovations/UiPath_Document_OCR/issues/1/comments",
        "events_url": "https://api.github.com/repos/mccm-innovations/UiPath_Document_OCR/issues/1/events",
        "html_url": "https://github.com/mccm-innovations/UiPath_Document_OCR/issues/1",
        "id": 488450349,
        "node_id": "MDU6SXNzdWU0ODg0NTAzNDk=",
        "number": 1,
        "title": "Using state_is_tuple=False probably make the detection slow",
        "user": {
            "login": "jialvarez",
            "id": 827495,
            "node_id": "MDQ6VXNlcjgyNzQ5NQ==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/827495?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jialvarez",
            "html_url": "https://github.com/jialvarez",
            "followers_url": "https://api.github.com/users/jialvarez/followers",
            "following_url": "https://api.github.com/users/jialvarez/following{/other_user}",
            "gists_url": "https://api.github.com/users/jialvarez/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jialvarez/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jialvarez/subscriptions",
            "organizations_url": "https://api.github.com/users/jialvarez/orgs",
            "repos_url": "https://api.github.com/users/jialvarez/repos",
            "events_url": "https://api.github.com/users/jialvarez/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jialvarez/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2019-09-03T07:59:03Z",
        "updated_at": "2019-09-04T17:49:22Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Hi, I'm checking this project and Amazon Rekognition to extract text from spanish DNI images.\r\n\r\nI've noticed this warning appearing many times:\r\n\r\n`WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f18cc56bc18>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\r\n`\r\nDetection is a bit slow even considering I'm using 8 CPU cores:\r\n\r\n```\r\nroot@tataogg:/home/neonigma/UiPath_Document_OCR/inference_server# ./serve\r\nStarting the inference server with 8 workers.\r\n[2019-09-03 07:49:57 +0000] [4946] [INFO] Starting gunicorn 19.9.0\r\n[2019-09-03 07:49:57 +0000] [4946] [INFO] Listening at: unix:/tmp/gunicorn_ocr.sock (4946)\r\n[2019-09-03 07:49:57 +0000] [4946] [INFO] Using worker: gevent\r\n[2019-09-03 07:49:57 +0000] [4950] [INFO] Booting worker with pid: 4950\r\n[2019-09-03 07:49:57 +0000] [4951] [INFO] Booting worker with pid: 4951\r\n[2019-09-03 07:49:57 +0000] [4952] [INFO] Booting worker with pid: 4952\r\n[2019-09-03 07:49:57 +0000] [4960] [INFO] Booting worker with pid: 4960\r\n[2019-09-03 07:49:57 +0000] [4968] [INFO] Booting worker with pid: 4968\r\n[2019-09-03 07:49:58 +0000] [4983] [INFO] Booting worker with pid: 4983\r\n[2019-09-03 07:49:58 +0000] [4984] [INFO] Booting worker with pid: 4984\r\n[2019-09-03 07:49:58 +0000] [4985] [INFO] Booting worker with pid: 4985\r\n\r\n```\r\n```\r\nroot@tataogg:/home/neonigma/UiPath_Document_OCR/inference_server# echo $MODEL_SERVER_TIMEOUT $MODEL_SERVER_WORKERS $NGINX_CONF_PATH $TEXT_RECOG_MODEL_PATH\r\n800 8 /home/neonigma/UiPath_Document_OCR/inference_server/nginx.conf /home/neonigma/UiPath_Document_OCR/inference_server/recog_model\r\n\r\n```\r\nI would like to speed up this recognition if possible:\r\n\r\n```\r\nINFO:Inference Server:Text recognition performed\r\nINFO:Inference Server:Execution time: 57.39146566390991\r\n\r\n```\r\nI'm not very familiar with Tensorflow nor machine learning at this moment, but I can write code in Python. If I can submit any patch, please let me know.\r\n\r\nThanks!"
    }
]