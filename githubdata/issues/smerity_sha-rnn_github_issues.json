[
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/11",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/11/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/11/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/11/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/11",
        "id": 555058613,
        "node_id": "MDU6SXNzdWU1NTUwNTg2MTM=",
        "number": 11,
        "title": "Results from Adaptive Span Transformer",
        "user": {
            "login": "djstrong",
            "id": 1849959,
            "node_id": "MDQ6VXNlcjE4NDk5NTk=",
            "avatar_url": "https://avatars1.githubusercontent.com/u/1849959?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/djstrong",
            "html_url": "https://github.com/djstrong",
            "followers_url": "https://api.github.com/users/djstrong/followers",
            "following_url": "https://api.github.com/users/djstrong/following{/other_user}",
            "gists_url": "https://api.github.com/users/djstrong/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/djstrong/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/djstrong/subscriptions",
            "organizations_url": "https://api.github.com/users/djstrong/orgs",
            "repos_url": "https://api.github.com/users/djstrong/repos",
            "events_url": "https://api.github.com/users/djstrong/events{/privacy}",
            "received_events_url": "https://api.github.com/users/djstrong/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2020-01-25T08:20:31Z",
        "updated_at": "2020-01-25T09:45:44Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "In the readme Adaptive Span Transformer is referred as small, but actually it is normal version. Small version achieves 1.3 BPC (BPB actually)."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/10",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/10/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/10/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/10/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/10",
        "id": 545056931,
        "node_id": "MDU6SXNzdWU1NDUwNTY5MzE=",
        "number": 10,
        "title": "SplitcrossEntropy",
        "user": {
            "login": "gslaller",
            "id": 45001587,
            "node_id": "MDQ6VXNlcjQ1MDAxNTg3",
            "avatar_url": "https://avatars1.githubusercontent.com/u/45001587?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gslaller",
            "html_url": "https://github.com/gslaller",
            "followers_url": "https://api.github.com/users/gslaller/followers",
            "following_url": "https://api.github.com/users/gslaller/following{/other_user}",
            "gists_url": "https://api.github.com/users/gslaller/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gslaller/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gslaller/subscriptions",
            "organizations_url": "https://api.github.com/users/gslaller/orgs",
            "repos_url": "https://api.github.com/users/gslaller/repos",
            "events_url": "https://api.github.com/users/gslaller/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gslaller/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2020-01-03T16:13:05Z",
        "updated_at": "2020-01-03T16:13:05Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Can you provide any further information on the loss function you are using? Perhaps a reference to a paper?"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/9/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/9",
        "id": 534263354,
        "node_id": "MDU6SXNzdWU1MzQyNjMzNTQ=",
        "number": 9,
        "title": "how to control GPU ram usage",
        "user": {
            "login": "jprobichaud",
            "id": 9254379,
            "node_id": "MDQ6VXNlcjkyNTQzNzk=",
            "avatar_url": "https://avatars2.githubusercontent.com/u/9254379?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jprobichaud",
            "html_url": "https://github.com/jprobichaud",
            "followers_url": "https://api.github.com/users/jprobichaud/followers",
            "following_url": "https://api.github.com/users/jprobichaud/following{/other_user}",
            "gists_url": "https://api.github.com/users/jprobichaud/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jprobichaud/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jprobichaud/subscriptions",
            "organizations_url": "https://api.github.com/users/jprobichaud/orgs",
            "repos_url": "https://api.github.com/users/jprobichaud/repos",
            "events_url": "https://api.github.com/users/jprobichaud/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jprobichaud/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2019-12-06T21:22:10Z",
        "updated_at": "2019-12-10T22:06:23Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Thanks for sharing this code!  I'd like to try on my own training dataset, but I keep getting GPU OOM problems:\r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 11.59 GiB (GPU 0; 11.91 GiB total capacity; 0 bytes already allocated; 11.43 GiB free; 0 bytes cached)\r\n```\r\n\r\nI've cut down the batch size to 8, emb size to 512, nhid to 2048 and nlayers to 2 and I still get the exact same message.\r\n\r\nMy training data set is 3.3GB (that's 1/10 of the data I would like to throw at it) so I'm already way over the enwik8 dataset (173MB) so I wonder where I should tweak the model/code..."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/8",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/8/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/8/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/8/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/8",
        "id": 533853230,
        "node_id": "MDU6SXNzdWU1MzM4NTMyMzA=",
        "number": 8,
        "title": "error happened when new token appears in the valid/test data set",
        "user": {
            "login": "carter54",
            "id": 26741594,
            "node_id": "MDQ6VXNlcjI2NzQxNTk0",
            "avatar_url": "https://avatars0.githubusercontent.com/u/26741594?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/carter54",
            "html_url": "https://github.com/carter54",
            "followers_url": "https://api.github.com/users/carter54/followers",
            "following_url": "https://api.github.com/users/carter54/following{/other_user}",
            "gists_url": "https://api.github.com/users/carter54/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/carter54/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/carter54/subscriptions",
            "organizations_url": "https://api.github.com/users/carter54/orgs",
            "repos_url": "https://api.github.com/users/carter54/repos",
            "events_url": "https://api.github.com/users/carter54/events{/privacy}",
            "received_events_url": "https://api.github.com/users/carter54/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2019-12-06T09:44:36Z",
        "updated_at": "2019-12-10T00:43:47Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Thanks first for such nice paper and work! \r\nI'm trying to train a text generation model with my own dataset. The tokenize function in data.py\r\nhttps://github.com/Smerity/sha-rnn/blob/218d748022dbcf32d50bbbb4d151a9b6de3f8bba/data.py#L34\r\nuses split() to tokenize sentence in the train dataset, and add token id in the dict. But in the valid/test dataset, the new tokens are neither added in the dict or tagged as an unknown token. Thus, the following error pop up. \r\n```\r\nProducing dataset...\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 121, in <module>\r\n    corpus = data.Corpus(args.data)\r\n  File \"/home/haha/sha-rnn/data.py\", line 31, in __init__\r\n    self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\r\n  File \"/home/haha/sha-rnn/data.py\", line 60, in tokenize\r\n    ids[token] = self.dictionary.word2idx[word]\r\nKeyError: 'bower_components'\r\n```\r\n\r\nDo you recommend to use other tokenize method (like word-piece) here?\r\n\r\nThanks again~"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/7",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/7/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/7/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/7/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/7",
        "id": 532593645,
        "node_id": "MDU6SXNzdWU1MzI1OTM2NDU=",
        "number": 7,
        "title": "Gradient overflows",
        "user": {
            "login": "stefan-it",
            "id": 20651387,
            "node_id": "MDQ6VXNlcjIwNjUxMzg3",
            "avatar_url": "https://avatars1.githubusercontent.com/u/20651387?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stefan-it",
            "html_url": "https://github.com/stefan-it",
            "followers_url": "https://api.github.com/users/stefan-it/followers",
            "following_url": "https://api.github.com/users/stefan-it/following{/other_user}",
            "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions",
            "organizations_url": "https://api.github.com/users/stefan-it/orgs",
            "repos_url": "https://api.github.com/users/stefan-it/repos",
            "events_url": "https://api.github.com/users/stefan-it/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stefan-it/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2019-12-04T10:34:47Z",
        "updated_at": "2019-12-10T00:30:30Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Hi @Smerity ,\r\n\r\nthanks for open sourcing the code for that great project :heart: \r\n\r\nI trained a character-based model for German on ~1GB of text (mainly from OPUS). It worked well for two epochs, but then the following error message is thrown:\r\n\r\n```bash\r\n| epoch   1 | 121090/129094 batches | lr 0.00200 | ms/batch 216.22 | loss  0.83 | ppl     2.28 | bpc    1.191\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\r\n| epoch   1 | 121100/129094 batches | lr 0.00200 | ms/batch 190.10 | loss   nan | ppl      nan | bpc      nan\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0\r\n\r\n\r\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0                                  \r\nTraceback (most recent call last):                                                                           \r\n  File \"main.py\", line 379, in <module>                                                                      \r\n    train(epoch - 1)                                                                                         \r\n  File \"main.py\", line 302, in train                                                                         \r\n    scaled_loss.backward()                                                                                   \r\n  File \"/usr/lib/python3.6/contextlib.py\", line 88, in __exit__                                              \r\n    next(self.gen)                                                                             \r\n  File \"/usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg/apex/amp/handle.py\", line 123, in scale_loss\r\n    optimizer._post_amp_backward(loss_scaler)                                                  \r\n  File \"/usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg/apex/amp/_process_optimizer.py\", line 241, in post_backward_no_master_weights\r\n    post_backward_models_are_masters(scaler, params, stashed_grads)                            \r\n  File \"/usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg/apex/amp/_process_optimizer.py\", line 127, in post_backward_models_are_masters\r\n    scale_override=(grads_have_scale, stashed_have_scale, out_scale))                          \r\n  File \"/usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg/apex/amp/scaler.py\", line 176, in unscale_with_stashed\r\n    out_scale/grads_have_scale,   # 1./scale,                                                  \r\nZeroDivisionError: float division by zero\r\n```\r\n\r\nThen I resumed training with a lower learning rate (pretty much the same parameters as stated in the main readme) and the same error is thrown after one epochs.\r\n\r\nDo you know how this can be prevented \ud83e\udd14\r\n\r\nHowever, the generated text is very interesting \ud83d\ude05\r\n\r\n```bash\r\n||||D i e _ P r o t o k o l l e , _ d i e _ a u f _ B e r e c h n u n g e n _ d e r _ F a m i l i e _ P a a r u n g _ u n d _ d e r _ E n t s c h e i d u n g e n _ f \u00fc r _ d i e _ R e g u l i e r u n g _ d e r _ U n i v e r s i t \u00e4 t _ v o r g e b r a c h t _ w e r d e n , _ w i r d _ d e m n a\r\nc h _ m i t _ n i e d r i g e n _ G e w i n n n i v e a u s _ d i s k u t i e r t _ w e r d e n .\r\nS i e _ k \u00f6 n n e n _ a u c h _ z w i s c h e n _ d e n _ v e r s c h i e d e n e n _ K o n z e p t i o n e n _ v o n _ F a m i l i e n _ u n d _ K i n d e r n _ i n t e r e s s i e r e n : _ b e i s p i e l s w e i s e : _ B i o g r a p h i e , _ M a g i e , _ G e s c h i c h t e , _ C a p t a\r\ni n _ S l a v i a - S t i l , _ A n s i c h t e n _ u n d _ V i d e o s .\r\nD i e s e r _ S c h a l l p e g e l _ l \u00e4 u f t _ i n _ e i n _ H \u00f6 h e n v e r s t e l l u n g s g e f \u00e4 \u00df _ d e s _ a l l g e m e i n e n _ G e r \u00e4 t e s _ b e i _ d e r _ D i c h t h e i t .\r\n||||A u f _ d e r _ W e s t s e i t e _ d e r _ A u t o b a h n _ A 1 , _ n a h e _ L a _ G o m e r a _ b e f i n d e n _ s i c h _ z w e i _ S t r a \u00df e n v e r b i n d u n g e n _ z w i s c h e n _ d e n _ B e r g e n _ u n d _ d e r _ S e h e n s w \u00fc r d i g k e i t .\r\nZ u _ d e n _ f o l g e n d e n _ D i e n s t l e i s t u n g e n _ g e h \u00f6 r e n _ T e l e f o n , _ k o s t e n l o s e _ P a r k p l \u00e4 t z e , _ e i n _ B \u00fc g e l e i s e n / - b r e t t _ ( 2 4 - S t u n d e n - R e z e p t i o n ) . \r\n```"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/6",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/6/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/6/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/6/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/6",
        "id": 530652436,
        "node_id": "MDU6SXNzdWU1MzA2NTI0MzY=",
        "number": 6,
        "title": "Requirements.txt",
        "user": {
            "login": "LanceNorskog",
            "id": 166777,
            "node_id": "MDQ6VXNlcjE2Njc3Nw==",
            "avatar_url": "https://avatars2.githubusercontent.com/u/166777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LanceNorskog",
            "html_url": "https://github.com/LanceNorskog",
            "followers_url": "https://api.github.com/users/LanceNorskog/followers",
            "following_url": "https://api.github.com/users/LanceNorskog/following{/other_user}",
            "gists_url": "https://api.github.com/users/LanceNorskog/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LanceNorskog/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LanceNorskog/subscriptions",
            "organizations_url": "https://api.github.com/users/LanceNorskog/orgs",
            "repos_url": "https://api.github.com/users/LanceNorskog/repos",
            "events_url": "https://api.github.com/users/LanceNorskog/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LanceNorskog/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2019-12-01T01:13:25Z",
        "updated_at": "2019-12-01T01:13:25Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Also, think about adding a Colab notebook. I've done all of my work there. It is (mostly) usable, and enforces a reproducible style of development."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/5",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/5/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/5/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/5/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/5",
        "id": 530385024,
        "node_id": "MDU6SXNzdWU1MzAzODUwMjQ=",
        "number": 5,
        "title": "Curious about 1-billion corpus perplexity",
        "user": {
            "login": "cmathx",
            "id": 5919493,
            "node_id": "MDQ6VXNlcjU5MTk0OTM=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5919493?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cmathx",
            "html_url": "https://github.com/cmathx",
            "followers_url": "https://api.github.com/users/cmathx/followers",
            "following_url": "https://api.github.com/users/cmathx/following{/other_user}",
            "gists_url": "https://api.github.com/users/cmathx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cmathx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cmathx/subscriptions",
            "organizations_url": "https://api.github.com/users/cmathx/orgs",
            "repos_url": "https://api.github.com/users/cmathx/repos",
            "events_url": "https://api.github.com/users/cmathx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cmathx/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2019-11-29T14:54:28Z",
        "updated_at": "2019-11-29T14:54:28Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "This neural network seems art. Waiting for more experiments in NLP tasks."
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/3/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/3",
        "id": 529874548,
        "node_id": "MDU6SXNzdWU1Mjk4NzQ1NDg=",
        "number": 3,
        "title": "Reproduced BPC of 1.077 using model with one attention layer",
        "user": {
            "login": "mkroutikov",
            "id": 14280777,
            "node_id": "MDQ6VXNlcjE0MjgwNzc3",
            "avatar_url": "https://avatars2.githubusercontent.com/u/14280777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mkroutikov",
            "html_url": "https://github.com/mkroutikov",
            "followers_url": "https://api.github.com/users/mkroutikov/followers",
            "following_url": "https://api.github.com/users/mkroutikov/following{/other_user}",
            "gists_url": "https://api.github.com/users/mkroutikov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mkroutikov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mkroutikov/subscriptions",
            "organizations_url": "https://api.github.com/users/mkroutikov/orgs",
            "repos_url": "https://api.github.com/users/mkroutikov/repos",
            "events_url": "https://api.github.com/users/mkroutikov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mkroutikov/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2019-11-28T12:08:33Z",
        "updated_at": "2019-11-28T16:45:51Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "```\r\n| epoch  13 | 11210/10986 batches | lr 0.00100 | ms/batch 188.01 | loss  0.71 | ppl     2.04 | bpc    1.029\r\n| epoch  13 | 11220/10986 batches | lr 0.00100 | ms/batch 188.13 | loss  0.64 | ppl     1.90 | bpc    0.925\r\n| epoch  13 | 11230/10986 batches | lr 0.00100 | ms/batch 193.46 | loss  0.73 | ppl     2.07 | bpc    1.052\r\n| epoch  13 | 11240/10986 batches | lr 0.00100 | ms/batch 193.60 | loss  0.74 | ppl     2.10 | bpc    1.071\r\n| epoch  13 | 11250/10986 batches | lr 0.00100 | ms/batch 193.58 | loss  0.71 | ppl     2.03 | bpc    1.021\r\n| epoch  13 | 11260/10986 batches | lr 0.00100 | ms/batch 185.65 | loss  0.73 | ppl     2.07 | bpc    1.051\r\n| epoch  13 | 11270/10986 batches | lr 0.00100 | ms/batch 194.74 | loss  0.72 | ppl     2.04 | bpc    1.032\r\n| epoch  13 | 11280/10986 batches | lr 0.00100 | ms/batch 194.90 | loss  0.67 | ppl     1.95 | bpc    0.964\r\n| epoch  13 | 11290/10986 batches | lr 0.00100 | ms/batch 180.78 | loss  0.73 | ppl     2.08 | bpc    1.057\r\n| epoch  13 | 11300/10986 batches | lr 0.00100 | ms/batch 193.61 | loss  0.73 | ppl     2.07 | bpc    1.050\r\n-----------------------------------------------------------------------------------------\r\n| end of epoch  14 | time: 2183.27s | valid loss  0.76 | valid ppl     2.15 | valid bpc    1.101\r\n-----------------------------------------------------------------------------------------\r\nModel total parameters: 53790932\r\n=========================================================================================\r\n| End of training | test loss  0.76 | test ppl     2.15 | test bpc    1.102\r\n=========================================================================================\r\n```\r\nAbove is the output at the end of the second training run, as in the README.\r\n\r\nMy setup: \r\n1. V100 GPU (AWS p3.2xlarge)\r\n2. Pytorch 1.3.1\r\n3. apex 0.1\r\n4. Followed training instructions from README: ran training twice 14 + 14 epochs, second time with smaller batch size and learning rate.\r\n\r\nTrained model is [here](https://s3.amazonaws.com/temp.innodatalabs.com/ENWIK8.pt)  (205Mb)\r\n\r\n## Other notes:\r\n* On the first training run (batch size 16) time per batch was 0.27 sec (faster than reported 0.7s on Titan V). GPU utilization is about 85%, with 70% memory used.\r\n* On the second run (batch size 8), time per batch was 0.19 sec. GPU utilization 80%, using about 40% of GPU memory\r\n\r\n"
    },
    {
        "url": "https://api.github.com/repos/Smerity/sha-rnn/issues/2",
        "repository_url": "https://api.github.com/repos/Smerity/sha-rnn",
        "labels_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/2/labels{/name}",
        "comments_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/2/comments",
        "events_url": "https://api.github.com/repos/Smerity/sha-rnn/issues/2/events",
        "html_url": "https://github.com/Smerity/sha-rnn/issues/2",
        "id": 529530357,
        "node_id": "MDU6SXNzdWU1Mjk1MzAzNTc=",
        "number": 2,
        "title": "Could you share pretrained model weights?",
        "user": {
            "login": "vhargitai",
            "id": 34478653,
            "node_id": "MDQ6VXNlcjM0NDc4NjUz",
            "avatar_url": "https://avatars1.githubusercontent.com/u/34478653?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vhargitai",
            "html_url": "https://github.com/vhargitai",
            "followers_url": "https://api.github.com/users/vhargitai/followers",
            "following_url": "https://api.github.com/users/vhargitai/following{/other_user}",
            "gists_url": "https://api.github.com/users/vhargitai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vhargitai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vhargitai/subscriptions",
            "organizations_url": "https://api.github.com/users/vhargitai/orgs",
            "repos_url": "https://api.github.com/users/vhargitai/repos",
            "events_url": "https://api.github.com/users/vhargitai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vhargitai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2019-11-27T19:16:28Z",
        "updated_at": "2019-12-05T09:49:52Z",
        "closed_at": null,
        "author_association": "NONE",
        "body": "Hi @Smerity , could you share the pretrained SHA-RNN weights from your WikiText103 experiments? I'd like to do some fine-tuning experiments with it for text classification. (It would be a huge help, as I only have access to an oven for limited time. :)) Thank you!"
    }
]