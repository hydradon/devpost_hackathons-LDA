<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li>
                        <div class="flex-video">
                            <iframe allowfullscreen="allowfullscreen" allowscriptaccess="always" class="video-embed" frameborder="0" height="371" mode="transparent" src="https://www.youtube.com/embed/18Iw4qYqfPo?enablejsapi=1&amp;hl=en_US&amp;rel=0&amp;start=&amp;version=3&amp;wmode=transparent" type="text/html" webkitallowfullscreen="true" width="660" wmode="transparent">
                            </iframe>
                        </div>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="838122" data-title="Fig 1. Demo: saliency maps with AlexNet" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/838/122/datas/original.png">
                            <img alt="FlashTorch &ndash; screenshot 1" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/838/122/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Fig 1. Demo: saliency maps with AlexNet
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="838123" data-title="Fig 2. Demo: activation maximization with VGG16" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/838/123/datas/original.png">
                            <img alt="FlashTorch &ndash; screenshot 2" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/838/123/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Fig 2. Demo: activation maximization with VGG16
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="833432" data-title="Fig 3. Saliency maps: visualizing what the pre-trained AlexNet is paying attention to within input images." href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/833/432/datas/original.png">
                            <img alt="FlashTorch &ndash; screenshot 3" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/833/432/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Fig 3. Saliency maps: visualizing what the pre-trained AlexNet is paying attention to within input images.
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="833436" data-title="Fig 4. Transfer learning (before additional training): the pre-trained network does not know where to focus in this flower." href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/833/436/datas/original.png">
                            <img alt="FlashTorch &ndash; screenshot 4" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/833/436/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Fig 4. Transfer learning (before additional training): the pre-trained network does not know where to focus in this flower.
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="833437" data-title="Fig 5. Transfer learning (after additional training): the network has learnt to focus on the mottled pattern!" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/833/437/datas/original.png">
                            <img alt="FlashTorch &ndash; screenshot 5" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/833/437/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Fig 5. Transfer learning (after additional training): the network has learnt to focus on the mottled pattern!
                            </i>
                        </p>
                    </li>
                </ul>
            </div>
            <div>
                <h1>
                    FlashTorch
                </h1>
                <p>
                    A Python visualization toolkit, built with PyTorch, for neural networks in PyTorch.
                </p>
                <p>
                    Neural networks are often described as "black box". The lack of understanding on how neural networks make predictions enables unpredictable/biased models, causing real harm to society and a loss of trust in AI-assisted systems.
                </p>
                <p>
                    Feature visualization is an area of research, which aims to understand how neural networks perceive images. However, implementing such techniques is often complicated.
                </p>
                <p>
                    FlashTorch was created to solve this problem!
                </p>
                <p>
                    You can apply feature visualization techniques (such as
                    <strong>
                        saliency maps
                    </strong>
                    and
                    <strong>
                        activation maximization
                    </strong>
                    ) on your model, with as little as a few lines of code. It is compatible with pre-trained models that come with
                    <a href="https://pytorch.org/docs/stable/torchvision/models.html" rel="nofollow">
                        torchvision
                    </a>
                    , and seamlessly integrates with other custom models built in PyTorch.
                </p>
                <h2>
                    Installation
                </h2>
                <p>
                    If you are installing FlashTorch for the first time:
                </p>
                <pre class="language-bash"><code>$ pip install flashtorch
</code></pre>
                <p>
                    Or to upgrade:
                </p>
                <pre class="language-bash"><code>$ pip install flashtorch -U
</code></pre>
                <h2>
                    How to use
                </h2>
                <p>
                    Below, you can find demo notebooks to get you started, with additional examples of using FlashTorch.
                </p>
                <h3>
                    Image handling (
                    <code>
                        flashtorch.utils
                    </code>
                    )
                </h3>
                <ul>
                    <li>
                        <a href="https://github.com/MisaOgura/flashtorch/blob/master/examples/examples/image_handling.ipynb" rel="nofollow">
                            Image handling
                        </a>
                        notebook
                    </li>
                </ul>
                <h3>
                    Saliency maps (
                    <code>
                        flashtorch.saliency
                    </code>
                    )
                </h3>
                <ul>
                    <li>
                        <a href="https://github.com/MisaOgura/flashtorch/blob/master/examples/visualize_saliency_with_backprop.ipynb" rel="nofollow">
                            Saliency map with backpropagation
                        </a>
                        notebook
                    </li>
                    <li>
                        <a href="https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/visualize_saliency_with_backprop_colab.ipynb" rel="nofollow">
                            Google Colab
                        </a>
                        version - best for trying it out
                    </li>
                </ul>
                <p>
                    <strong>
                        <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)" rel="nofollow">
                            Saliency
                        </a>
                    </strong>
                    in human visual perception is a
                    <em>
                        subjective quality
                    </em>
                    that makes certain things within the field of view
                    <em>
                        stand out
                    </em>
                    from the rest and
                    <em>
                        grabs our attention
                    </em>
                    .
                </p>
                <p>
                    <strong>
                        <a href="https://arxiv.org/pdf/1312.6034.pdf" rel="nofollow">
                            Saliency maps
                        </a>
                    </strong>
                    in computer vision provide indications of the most salient regions within images. By creating a saliency map for neural networks, we can gain some intuition on
                    <em>
                        "where the network is paying the most attention to"
                    </em>
                    in an input image.
                </p>
                <p>
                    <strong>
                        See Fig 1.
                    </strong>
                    (from the image gallery) for the demonstration of how to create saliency maps with AlexNet.
                </p>
                <h3>
                    Activation maximization (
                    <code>
                        flashtorch.activmax
                    </code>
                    )
                </h3>
                <ul>
                    <li>
                        <a href="https://github.com/MisaOgura/flashtorch/blob/master/examples/activation_maximization.ipynb" rel="nofollow">
                            Activation maximization
                        </a>
                        notebook
                    </li>
                    <li>
                        <a href="https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/activation_maximization_colab.ipynb" rel="nofollow">
                            Google Colab
                        </a>
                        version - best for trying it out
                    </li>
                </ul>
                <p>
                    <strong>
                        <a href="https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf" rel="nofollow">
                            Activation maximization
                        </a>
                    </strong>
                    is one form of feature visualization that allows us to visualize what CNN filters are "looking for", by applying each filter to an input image and updating the input image to maximize the activation of the filter of interest (i.e. treating it as a gradient ascent task with filter activation values as the loss).
                </p>
                <p>
                    Using
                    <code>
                        flashtorch.activmax
                    </code>
                    module, let's visualize images optimized with filters
from
                    <a href="https://arxiv.org/pdf/1409.1556.pdf" rel="nofollow">
                        VGG16
                    </a>
                    pre-trained on
                    <a href="http://www.image-net.org/" rel="nofollow">
                        ImageNet
                    </a>
                    classification tasks.
                </p>
                <p>
                    <strong>
                        See Fig 2.
                    </strong>
                    (from the image gallery) for the demonstration of how to perform activation maximization with VGG16.
                </p>
                <p>
                    Concepts such as
                    <em>
                        'eyes'
                    </em>
                    (filter 45) and
                    <em>
                        'entrances (?)'
                    </em>
                    (filter 271) seem to appear in the conv5_1 layer of VGG16.
                </p>
                <p>
                    Visit the notebook above to see what earlier layers do!
                </p>
                <h2>
                    Author
                </h2>
                <h3>
                    Misa Ogura
                </h3>
                <p>
                    <a href="https://github.com/MisaOgura" rel="nofollow">
                        Github
                    </a>
                    |
                    <a href="https://medium.com/@misaogura" rel="nofollow">
                        Medium
                    </a>
                    |
                    <a href="https://twitter.com/misa_ogura" rel="nofollow">
                        twitter
                    </a>
                    |
                    <a href="https://www.linkedin.com/in/misaogura/" rel="nofollow">
                        LinkedIn
                    </a>
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag">
                            matplotlib
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/numpy">
                                numpy
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/pytorch">
                                pytorch
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/MisaOgura/flashtorch" rel="nofollow" target="_blank" title="https://github.com/MisaOgura/flashtorch">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                    <li>
                        <a href="https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/visualize_saliency_with_backprop_colab.ipynb" rel="nofollow" target="_blank" title="https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/visualize_saliency_with_backprop_colab.ipynb">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                colab.research.google.com
                            </span>
                        </a>
                    </li>
                    <li>
                        <a href="https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/activation_maximization_colab.ipynb" rel="nofollow" target="_blank" title="https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/activation_maximization_colab.ipynb">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                colab.research.google.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
