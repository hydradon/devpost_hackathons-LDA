<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li class="text-center">
                        <a data-lightbox="758817" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/758/817/datas/original.jpg">
                            <img alt="Gesture Control(Hands free mouse*) &ndash; screenshot 1" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/758/817/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                            </i>
                        </p>
                    </li>
                </ul>
            </div>
            <div>
                <h2>
                    Inspiration
                </h2>
                <p>
                    We began developing a basic gesture control device to help us navigate about the desktop UI. However while we were messing around with the sensor calibration and movement detection we found that it was a rather simple yet intuitive mouse input. So we decided to design a form of a hands free mouse that would complement the wide range of IO devices available to the differently abled rather than replace them.
                </p>
                <h2>
                    What it does
                </h2>
                <p>
                    Using a combination of a gyroscope and an accelerometer we were able to determine the pitch, yaw and roll of the hand. With this data we could identify the motion of the hand and hence run scripts accordingly.
                </p>
                <h2>
                    How we built it
                </h2>
                <p>
                    We used an Arduino with a built in gyroscope and accelerometer to plot the data points, we then calculated the standard deviation to identify movement along any of the three axis. We then print a message to the serial port according to the recognized gesture. Python will then run the script according to the serial values sent to it.
                </p>
                <h2>
                    Challenges we ran into
                </h2>
                <p>
                    The main challenge that we ran into was identifying what would be classified as a given gesture as the sensor would often fluctuate and gradually vary over time rendering most of our baseline thresholds worthless. We counteracted this by implementing a calibration factor which will help in the normalization of the data.
                </p>
                <h2>
                    Accomplishments that we're proud of
                </h2>
                <p>
                    We were proud of the fact that this was our very first hackathon and we were able to set our sights on a reasonable goal and through strict time management we were able to complete well ahead of schedule allowing us to optimize the algorithm we used for identifying gestures.
                </p>
                <h2>
                    What we learned
                </h2>
                <p>
                    We learned how to use libraries specific to the arduino genuini 101 and the how to communicate between the arduino and python, this allowed us to handle data collection using the arduino and plotting and scripting.
                </p>
                <h2>
                    What's next for Gesture Control(Hands free mouse*)
                </h2>
                <p>
                    We believe the next natural progression for the gesture control would to develop a front end either through a website or an android app which would allow us to calibrate the sensor and configure different gestures to run different scripts. Another major task ahead of us would be to produce a library for the data collection allowing more people to devise more intuitive and capable gesture recognition(eagerly hoping that AI and ML could be used to enhance the gesture control drastically)
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/c">
                                c
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/joeKonathapally/gestureControlMouse.git" rel="nofollow" target="_blank" title="https://github.com/joeKonathapally/gestureControlMouse.git">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
