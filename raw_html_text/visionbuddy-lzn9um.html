<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div>
                <h2>
                    Inspiration
                </h2>
                <p>
                    We are inspired by the potential of powerful computer vision systems to _ make a difference _ in the life of individuals with visual impairments.
                </p>
                <h2>
                    What it does
                </h2>
                <p>
                    VisionBuddy is an application that helps individuals with visual impairments get a vocal description of the objects in their surroundings. When the application starts, the user is vocally instructed to press long on the screen to get a description of the environment. Since VisionBuddy is a fully immersive Android application, the user can press anywhere on the screen to activate the vocal description. The application can identify the categories found in the
                    <a href="http://cocodataset.org/#home" rel="nofollow">
                        COCO dataset
                    </a>
                    .
                </p>
                <h2>
                    How we built it
                </h2>
                <p>
                    VisionBuddy is based on
                    <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java" rel="nofollow">
                        TF Detect
                    </a>
                    , which uses
                    <a href="https://github.com/tensorflow/models/tree/master/research/object_detection/" rel="nofollow">
                        Tensorflow Object Detection API
                    </a>
                    as published in
                    <a href="https://arxiv.org/abs/1611.10012" rel="nofollow">
                        Speed/accuracy trade-offs for modern convolutional object detectors
                    </a>
                    to "localize and track objects (from 80 categories) in the camera preview in real-time".
                </p>
                <h2>
                    Challenges we ran into
                </h2>
                <p>
                    Figuring out the camera ratio to fit the whole screen in order to give an immersive experience was not straightforward. Integrating vocal instructions was challenging.
                </p>
                <h2>
                    Accomplishments that we're proud of
                </h2>
                <p>
                    Getting a tensorflow model to run on a mobile device!
                </p>
                <h2>
                    What we learned
                </h2>
                <p>
                    We expanded our knowledge in Android programming and tensorflow.
                </p>
                <h2>
                    What's next for VisionBuddy
                </h2>
                <p>
                    The next step is to add a Visual Question Answering agent to VisionBuddy. We are also interested in adding a localization feature that "remembers" the location of the objects of interests.
                </p>
            </div>
        </div>
    </body>
</html>
