<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li>
                        <div class="flex-video">
                            <iframe allowfullscreen="allowfullscreen" allowscriptaccess="always" class="video-embed" frameborder="0" height="371" mode="transparent" src="https://www.youtube.com/embed/YaD56kijnGs?enablejsapi=1&amp;hl=en_US&amp;rel=0&amp;start=&amp;version=3&amp;wmode=transparent" type="text/html" webkitallowfullscreen="true" width="660" wmode="transparent">
                            </iframe>
                        </div>
                    </li>
                </ul>
            </div>
            <div>
                <h1>
                    3D Material Synthesis
                </h1>
                <p>
                    Estimate high resolution surface normals from a single view using multiple light sources. This convolutional approach is used to the estimate the surface geometry for photo-realistic rendering. Upload photos from your mobile phone and create materials for Unity3D or Blender.
                </p>
                <h2>
                    Dependencies
                </h2>
                <ul>
                    <li>
                        <a href="https://unity.com/" rel="nofollow">
                            Unity3D
                        </a>
                    </li>
                    <li>
                        <a href="https://www.anaconda.com/distribution/" rel="nofollow">
                            Python 3+
                        </a>
                        (tensorflow, numpy, Pillow, matplotlib)
                    </li>
                </ul>
                <h2>
                    Generate Data with the
                    <a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@7.0/manual/index.html" rel="nofollow">
                        Universal Render Pipeline
                    </a>
                    in Unity3D
                </h2>
                <p>
                    4 light sources are placed around a scene to mimic LEDs on a tripod-like structure
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/Screenshot.png?raw=true" src="https://res.cloudinary.com/devpost/image/fetch/s---6KwMwkq--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/Screenshot.png%3Fraw%3Dtrue">
                </p>
                <p>
                    The lights turn on individually and a screenshot is captured for each. A labeled data set is created with the simulated images and their corresponding normal map.
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/animation.gif?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--E9R7r7sY--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/animation.gif%3Fraw%3DTrue">
                </p>
                <p>
                    The normal map encodes information about how bumpy or curved the surface is so that light can interact with it in a realistic manner. More information about normal maps be found here:
                    <a href="https://docs.unity3d.com/Manual/StandardShaderMaterialParameterNormalMap.html" rel="nofollow">
                        https://docs.unity3d.com/Manual/StandardShaderMaterialParameterNormalMap.html
                    </a>
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/NormalSurface.png?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--ijaTgv8F--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/NormalSurface.png%3Fraw%3DTrue">
                </p>
                <p>
                    Here is the difference between a surface with and without a normal map while being illuminated with a directional light at 45 degrees
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/normal_anim.gif?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--e6np80WU--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/normal_anim.gif%3Fraw%3DTrue">
                </p>
                <h2>
                    Finding Textures Online
                </h2>
                <p>
                    The training data is composed of high-resolution textures with normals from sources like
                    <a href="https://www.substance3d.com/" rel="nofollow">
                        https://www.substance3d.com/
                    </a>
                </p>
                <p>
                    A web crawler is created to find training data on websites that provide free textures
                </p>
                <ul>
                    <li>
                        <a href="https://3dtextures.me" rel="nofollow">
                            https://3dtextures.me
                        </a>
                    </li>
                    <li>
                        <a href="https://cc0textures.com/" rel="nofollow">
                            https://cc0textures.com/
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cgbookcase.com/" rel="nofollow">
                            https://www.cgbookcase.com/
                        </a>
                    </li>
                    <li>
                        <a href="https://texturehaven.com/textures/" rel="nofollow">
                            https://texturehaven.com/textures/
                        </a>
                    </li>
                </ul>
                <p>
                    To use the script follow:
                </p>
                <pre class="language-nolang"><code>python webscrape.py --BASE_URL https://3dtextures.me/ --PATTERN https://drive 
</code></pre>
                <p>
                    To download folders from a list of google drive links use:
                </p>
                <pre class="language-nolang"><code>python gdrive_download.py --file download_links.txt --dir train
</code></pre>
                <p>
                    Format the data and then import the directory
                    <code>
                        train/Textures/
                    </code>
                    into Unity
                </p>
                <pre class="language-nolang"><code>python format_data.py   
</code></pre>
                <p>
                    Use the script
                    <code>
                        ScreenCapture.cs
                    </code>
                    within Unity to generate training samples for a CNN. The training data is augmented within Unity to account for different perspectives &amp; small distortions (e.g. rotations, translations and cropping). Set the file path before running the "TrainingSamples" scene. Ignore all moments Unity tries to convert the texture type to a normal map. The normal map will be set to the albedo/base map in order to generate a ground truth label. The rendering is weird when generating a ground-truth if the normal map texture type is set to "normal map", just keep it as "default".
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/unity_training.gif?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--y21cggbb--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/unity_training.gif%3Fraw%3DTrue">
                </p>
                <h2>
                    Machine learning model
                </h2>
                <p>
                    INPUT: 4 images 480 x 320 corresponding to light from 4 different angles
                </p>
                <p>
                    OUTPUT: 1 image 480 x 320 px corresponding to a normal map
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/texture_anim.gif?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--mCP-gZxv--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/texture_anim.gif%3Fraw%3DTrue">
                </p>
                <p>
                    The architecture of the neural network consists of a few convolutional layers, it looks more complex than it is due to the multiple inputs.
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/encoder.png?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--vyauAULb--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/encoder.png%3Fraw%3DTrue">
                </p>
                <p>
                    The training was done by optimizing for the mean squared error using Adam with a batch size of 8 images. The neural network was trained with 1500 samples for 20 epochs on a GTX 1070 (total training time ~30 minutes). A total of 12,035 trainable parameters.
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/normal_training.png?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--lrvOPGx5--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/normal_training.png%3Fraw%3DTrue">
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/model_validation.png?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--lG3ZEEeG--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/model_validation.png%3Fraw%3DTrue">
                </p>
                <h2>
                    Use Cases
                </h2>
                <p>
                    An LED strip with an arduino can simulate the training environment from Unity. A pyramid structure is required to stablize a camera and position the LED strip. A tutorial for how to construct a PVC frame is coming soon... Here is the first light with the Arduino:
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/arduino_test1.jpg?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--CxQw05HW--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/arduino_test1.jpg%3Fraw%3DTrue">
                </p>
                <p>
                    A sequence of picutres from a cell phone can be uploaded for the model to perform inference on
                </p>
                <pre class="language-python"><code>from PIL import Image
import matplotlib.pyplot as plt
from model_train import build_cnn_encoder

if __name__ == "__main__":

img = np.asarray(Image.open("test.jpg"))

encoder = build_cnn_encoder( 
input_dims=[(img.size[1],img.size[0],3)]*4, 
layer_sizes=[ (8,8,8) ]*4,
combined_layers = [8,8,8], 
output_dim=(img.size[1],img.size[0],3)
)

encoder.load_weights("encoder_weights.h5") 
output = encoder.predict([X0,X1,X2,X3])

</code></pre>
                <p>
                    The normal maps can then be rendered in Unity by creating a new material and setting the Base Map and Normal Map fields
                </p>
                <p>
                    <img alt="" data-canonical-url="https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/arduino_test2.jpg?raw=True" src="https://res.cloudinary.com/devpost/image/fetch/s--qEtY_6aC--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/smaerdlatigid/3D-Material-Synthesis/blob/master/Figures/arduino_test2.jpg%3Fraw%3DTrue">
                </p>
                <p>
                    An Arduino Uno with a NeoPixel LED strip is used to capture images with a mobile phone. The 
LED Strip is:
                    <a href="https://www.adafruit.com/product/2562" rel="nofollow">
                        https://www.adafruit.com/product/2562
                    </a>
                    . More information about getting started with the NeoPixel Strip can be found here:
                    <a href="https://learn.adafruit.com/adafruit-neopixel-uberguide" rel="nofollow">
                        https://learn.adafruit.com/adafruit-neopixel-uberguide
                    </a>
                    . The code for the strip is here:
                </p>
                <h3>
                    License
                </h3>
                <p>
                    This software is intended strictly for educational purposes. Please cite this work if you use any of these algorithms for your project. Licensing fees may apply for any use beyond educational purposes, please contact
                    <a href="mailto:support@digitaldream.io" rel="nofollow">
                        support@digitaldream.io
                    </a>
                    for more information
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/unity">
                                unity
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/smaerdlatigid/3D-Material-Synthesis" rel="nofollow" target="_blank" title="https://github.com/smaerdlatigid/3D-Material-Synthesis">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
