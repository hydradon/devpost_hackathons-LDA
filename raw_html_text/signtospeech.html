<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li>
                        <div class="flex-video">
                            <iframe allowfullscreen="allowfullscreen" allowscriptaccess="always" class="video-embed" frameborder="0" height="371" mode="transparent" src="https://www.youtube.com/embed/yhQkUlE-8uM?enablejsapi=1&amp;hl=en_US&amp;rel=0&amp;start=&amp;version=3&amp;wmode=transparent" type="text/html" webkitallowfullscreen="true" width="660" wmode="transparent">
                            </iframe>
                        </div>
                    </li>
                </ul>
            </div>
            <div>
                <h2>
                    Inspiration
                </h2>
                <p>
                    In Hackprinceton 2018, Christian worked with a deaf/mute student to create an AR application to caption sign language. They failed. However, his new friend's story was inspiring. During their pitch, he said that growing up deaf was lonely because having to sign to talk was isolating. If applications like what we had been planning to make existed it would have helped many other hearing impaired people like him.
                </p>
                <p>
                    For the first time in history, a group of undergraduates who learned from free intro to ML videos could create technology advanced enough to challenge impairments and impact lives meaningfully.
                </p>
                <p>
                    So we built a first iteration as a proof of concept for a robust generalized sign language model through keypoint estimation. To make this model useful in the short-term, we'll be tackling the accessibility problem with smart homes by synthesizing speech to issue voice commands.
                </p>
                <h2>
                    What it does
                </h2>
                <ol>
                    <li>
                        It detects your body's keypoints (finds where your head, hands, fingers etc... in 2d space)
                    </li>
                    <li>
                        We feed these keypoints into our deep learning model to see what gesture you're most likely trying to do
                    </li>
                    <li>
                        We then figure out what command your gesture maps to, and use google cloud text to speech to send commands to any smart home device
                    </li>
                </ol>
                <h2>
                    How we built it
                </h2>
                <p>
                    We used openpose to estimate keypoints and pytorch to build the deep learning model. We then used Google speech to text to synthesize a voice command to Alexa
                </p>
                <h2>
                    Challenges we ran into
                </h2>
                <ul>
                    <li>
                        Hardware challenges
                    </li>
                    <li>
                        Data, data, data
                    </li>
                    <li>
                        Time
                    </li>
                </ul>
                <h2>
                    Accomplishments that we're proud of
                </h2>
                <p>
                    Robust model that works on any type of camera (web cam, phone cam etc...) on any type of background, and do not require any special equipment.
                </p>
                <p>
                    Most modern deep learning approaches require expensive equipment or rely on rigid constraints such as solid colored backgrounds. This works in the wild!
                </p>
                <h2>
                    What we learned
                </h2>
                <p>
                    Babysitting the training process, pytorch, deep learning concepts, teamwork
                </p>
                <h2>
                    What's next for SignToSpeech
                </h2>
                <p>
                    This project shows that with good data a generalized sign/gesture to text model could work. This opens up a whole new world of innovation for HCI (more gesture based UI), AR, and of course a potential novel solution to the sign language computer vision problem.
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/c">
                                c
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            jupyter-notebooks
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            openpose
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/pytorch">
                                pytorch
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/christian-rncl/bitcamp2019" rel="nofollow" target="_blank" title="https://github.com/christian-rncl/bitcamp2019">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
