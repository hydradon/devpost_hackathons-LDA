<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li class="text-center">
                        <a data-lightbox="830250" data-title="gossipsub" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/830/250/datas/original.png">
                            <img alt="Berlin GossipSub profiling &ndash; screenshot 1" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/830/250/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                gossipsub
                            </i>
                        </p>
                    </li>
                </ul>
            </div>
            <div>
                <h1>
                    EthBerlinZwei: Profiling Libp2p Gossipsub, Golang version
                </h1>
                <p>
                    Hackathon submission by @protolambda, learning libp2p with a non-networking background.
                </p>
                <p>
                    Short presentation:
                    <a href="https://docs.google.com/presentation/d/1lL4QiPgbdDVIp9sSp7KL2oZ6Oquis-rCGU3MUkTX9BU/edit?usp=sharing" rel="nofollow">
                        https://docs.google.com/presentation/d/1lL4QiPgbdDVIp9sSp7KL2oZ6Oquis-rCGU3MUkTX9BU/edit?usp=sharing
                    </a>
                </p>
                <h2>
                    The bounty problem
                </h2>
                <blockquote>
                    <p>
                        Find and fix bottlenecks and performance hotspots in the Go implementation of gossipsub.
                    </p>
                </blockquote>
                <p>
                    See
                    <a href="https://github.com/ethberlinzwei/Bounties/issues/18" rel="nofollow">
                        this issue on
                        <code>
                            bounties/EthBerlinZwei
                        </code>
                    </a>
                    .
                </p>
                <p>
                    And so there it starts; read up on libp2p knowledge, 
read the
                    <a href="https://github.com/libp2p/specs/tree/master/pubsub/gossipsub" rel="nofollow">
                        Gossipsub spec
                    </a>
                    and then trial-and-error throughout the hackathon. I started late however, since I worked on other Eth 2 issues too.
Thanks to @raulk for getting me up to speed to work on this so fast.
                </p>
                <p>
                    Note that this is a hack, produced with a "see it work first" mindset, not a research paper. You are welcome to fork and improve the profiling.
                </p>
                <h2>
                    Approach
                </h2>
                <p>
                    To profile anything at all, some kind of test-run is necessary.
One that stresses go-libp2p with a high throughput, with a good amount of peers and topics.
Then, a PPROF profile can be made of the test-run, and help identify hotspots to optimize.
                </p>
                <h3>
                    Why not use benchmarking?
                </h3>
                <p>
                    Since the task is not to benchmark libp2p (discussed options here with @raulk however), but to profile and find (and fix) the hotspots,
a more practical test-run with the actual overhead of opening a connection and not sharing memory for messages helps identify hotspots.
                </p>
                <p>
                    Also, the message-interval and size parameters are less strict: they can definitely affect speed,
 but there are only so many extremes to find hotspots for.
                </p>
                <p>
                    Benchmarking of the isolated gossipsub logic would be better if done with a mock net,
                    <a href="https://github.com/libp2p/go-libp2p/blob/master/p2p/net/mock/mock_net.go" rel="nofollow">
                        something like this
                    </a>
                    This however hides the overhead introduced by passing messages to a real socket, skewing the priorities in what to optimize for.
If practical issues are solved, one could then use Perf to profile a Go benchmark with, and look into the memory allocations and flamegraph of the remaining calls.
The bigger picture found in call-graphs in a non-benchmark setting does not show gossipsub code itself to be the bottleneck in practice however, hence not going the benchmarking route.
                </p>
                <h3>
                    Profiling settings
                </h3>
                <p>
                    Common settings for the produced
                    <em>
                        hackathon results
                    </em>
                    (not claiming perfectness, time constraints to for pretty parametrization apply):
                </p>
                <pre class="language-go"><code>// total hosts
hostCount := 100
// peers per host (randomly assigned)
degree := 10

// pubsub topic chances:
"/libp2p/example/berlin/protolambda/foo":  0.7,
"/libp2p/example/berlin/protolambda/bar":  0.4,
"/libp2p/example/berlin/protolambda/quix": 0.8,

// A no-op logger is used during benchmarking for speed.
logger := zwei.NewDebugLogger(nil)
// For debugging this can be changed to:  
// logger := zwei.NewDebugLogger(log.New(os.Stdout, "experiment: ", log.Lmicroseconds))

// message size
// big: 8 - 15 KB
minMsgByteLen := 8 &lt;&lt; 10
maxMsgByteLen := 16 &lt;&lt; 10
// small: 10 bytes
minMsgByteLen := 10
maxMsgByteLen := 10

// publish interval range for each simulated host (publish on 1 random topic)
minSleepMs := 100
maxSleepMs := 300

// libp2p settings
// transport:
libp2p.Transport(tcp.NewTCPTransport),
// mux choice:
libp2p.Muxer("/yamux/1.0.0", yamux.DefaultTransport),
//libp2p.Muxer("/mplex/6.7.0", mplex.DefaultTransport), // for some later profiles with mplex
// security:
libp2p.Security(secio.ID, secio.New),

// GossipSub settings
// Initially true, signing with Secp256k1.
// Later disabled, since this was the biggest practical bottleneck, and obfuscates the smaller differences. 
pubsub.WithMessageSigning(true)

// loopback through localhost, with no artificial latency
libp2p.ListenAddrStrings(
"/ip4/127.0.0.1/tcp/0", // 0: gets a random port assigned on localhost
),

// There also are options to change the RNG seed for both initialization and the testrun itself,
// but libp2p (interaction with machine itself, and go-routine scheduling) is not deterministic enough
// to make the results fully reproducible. 
</code></pre>
                <h2>
                    Usage
                </h2>
                <ol>
                    <li>
                        Configure
                        <code>
                            main.go
                        </code>
                        options: a
                        <code>
                            zwei.Experiment
                        </code>
                        is created with these.
Message length and interval can be changed in the experiment code, if required.
                    </li>
                    <li>
                        PPROF CPU-Profiling starts after setting up the experiment (starting hosts, starting gossipsub, and subscribing to topics)
                        <br>
                    </li>
                    <li>
                        Start experiment
                    </li>
                    <li>
                        Wait for stop-signal
                    </li>
                    <li>
                        Stop profiling, save results, see log output for profiling output location.
                    </li>
                    <li>
                        Stop libp2p tasks and close resources with
                        <code>
                            Experiment.Close()
                        </code>
                    </li>
                </ol>
                <p>
                    To generate a call-graph:
                </p>
                <pre class="language-bash"><code>go tool pprof -web /tmp/profile......../cpu.pprof 
</code></pre>
                <h2>
                    Profiling results
                </h2>
                <p>
                    Early results settings: Yamux, signed GossipSub, small 10 byte messages: signature verification is the clear bottleneck.
                </p>
                <p>
                    This test run published 31k messages, 1500k were received. A 145 seconds run.
                    <a href="results/pprof_31k_published_1500k_received.svg" rel="nofollow">
                        Full callgraph SVG
                    </a>
                </p>
                <p>
                    Then, I disabled GossipSub signatures (
                    <code>
                        pubsub.WithMessageSigning(false)
                    </code>
                    ) to see what was left.
                </p>
                <p>
                    For small messages, it is Yamux triggering secio encryption, which then writes to a socket connection provided by the kernel, which also forms a bottleneck.
                </p>
                <p>
                    This test run published 8k messages, 600k were received. A 20 seconds run.
                    <a href="results/pprof_no_gossipsub_signing_8k_published_600k_received.svg" rel="nofollow">
                        Full callgraph SVG
                    </a>
                </p>
                <p>
                    Raul then recommended to increase the message size, so repeat this with random 8 - 16 KB messages:
                </p>
                <p>
                    This test run published 5k messages, 200k were received. A 90 seconds run. Note the significantly lower throughput.
                    <a href="results/pprof_no_gossipsub_signing_5k_published_200k_received_8k_to_16k_bytelen_yamux.svg" rel="nofollow">
                        Full callgraph SVG
                    </a>
                </p>
                <p>
                    For larger messages, SHA-256 calls by Yamux become the bottleneck.
However, it looks like it is already using the excellent
                    <a href="https://github.com/minio/sha256-simd" rel="nofollow">
                        Sha-256 SIMD library
                    </a>
                    for speed,
so there is not much to gain unless something is being hashed twice and can be cached.
                </p>
                <p>
                    Now try again with Mplex:
                </p>
                <p>
                    This test run published 8k messages, 357k were received. A 33 seconds run. Note the significantly lower throughput.
                    <a href="results/pprof_no_gossipsub_signing_8k_published_375k_received_8k_to_16k_bytelen_mplex.svg" rel="nofollow">
                        Full callgraph SVG
                    </a>
                </p>
                <p>
                    SHA-256 (and general secio crypto) is still by far the biggest bottleneck.
                </p>
                <h2>
                    Conclusion
                </h2>
                <p>
                    GossipSub itself is primarily limited by the crypto necessary to verify and encrypt the messages,
and the data-structures used in its implementation do not seem to be worth optimizing at this time.
                </p>
                <p>
                    There seems to be some interesting difference in mplex vs. yamux to look into at a later moment, if it is not a usage problem from my side.
                </p>
                <h2>
                    LICENSE
                </h2>
                <p>
                    MIT, see
                    <a href="./LICENSE" rel="nofollow">
                        <code>
                            LICENSE
                        </code>
                        file
                    </a>
                    .
Some initial code was adapted from the go-libp2p examples repository,
                    <a href="https://github.com/libp2p/go-libp2p-examples" rel="nofollow">
                        here
                    </a>
                    ,
                    <a href="https://github.com/libp2p/go-libp2p-examples/blob/master/LICENSE" rel="nofollow">
                        also licensed with MIT
                    </a>
                    .
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/go">
                                go
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/protolambda/go-libp2p-gossip-berlin" rel="nofollow" target="_blank" title="https://github.com/protolambda/go-libp2p-gossip-berlin">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
