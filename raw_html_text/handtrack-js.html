<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li class="text-center">
                        <a data-lightbox="803367" data-title="Handtrack.js demo interface for tracking hands from live webcam feed and static images." href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/367/datas/original.png">
                            <img alt="Handtrack.js  &ndash; screenshot 1" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/367/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Handtrack.js demo interface for tracking hands from live webcam feed and static images.
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="803368" data-title="Handtrack.js demo interface for tracking hands from videos." href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/368/datas/original.png">
                            <img alt="Handtrack.js  &ndash; screenshot 2" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/368/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Handtrack.js demo interface for tracking hands from videos.
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="803365" data-title="Example Codepen that uses handtrack.js to control the paddles on a pong game via webcam video" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/365/datas/original.png">
                            <img alt="Handtrack.js  &ndash; screenshot 3" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/365/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Example Codepen that uses handtrack.js to control the paddles on a pong game via webcam video
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="803366" data-title="Documentation on usage for Handtrack.js" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/366/datas/original.png">
                            <img alt="Handtrack.js  &ndash; screenshot 4" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/803/366/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Documentation on usage for Handtrack.js
                            </i>
                        </p>
                    </li>
                </ul>
            </div>
            <div>
                <h2>
                    Inspiration
                </h2>
                <blockquote>
                    <p>
                        View a live demo in your
                        <a href="https://victordibia.github.io/handtrack.js/" rel="nofollow">
                            browser here
                        </a>
                        . Modify it in Codepen
                        <a href="https://codepen.io/victordibia/full/aGpRZV" rel="nofollow">
                            here
                        </a>
                    </p>
                </blockquote>
                <p>
                    <a href="https://victordibia.github.io/handtrack.js/" rel="nofollow">
                        <img data-canonical-url="https://github.com/victordibia/handtrack.js/raw/master/demo/images/bossflip.gif" src="https://res.cloudinary.com/devpost/image/fetch/s--2Or4OGZ7--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/victordibia/handtrack.js/raw/master/demo/images/bossflip.gif">
                    </a>
                </p>
                <p>
                    Handtrack.js is a library for prototyping realtime hand detection (bounding box), directly in the browser. Underneath, it uses a trained convolutional neural network that provides bounding box predictions for the location of hands in an image. The convolutional neural network (ssdlite, mobilenetv2) is trained using the tensorflow object detection api (
                    <a href="https://github.com/victordibia/handtracking/issues" rel="nofollow">
                        see here
                    </a>
                    ).
                </p>
                <table class="responsive">
                    <thead>
                        <tr>
                            <th>
                                FPS
                            </th>
                            <th>
                                Image Size
                            </th>
                            <th>
                                Device
                            </th>
                            <th>
                                Browser
                            </th>
                            <th>
                                Comments
                            </th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>
                                21
                            </td>
                            <td>
                                450 * 380
                            </td>
                            <td>
                                Macbook Pro (i7, 2.2GHz, 2018)
                            </td>
                            <td>
                                Chrome Version 72.0.3626
                            </td>
                            <td>
                                --
                            </td>
                        </tr>
                        <tr>
                            <td>
                                14
                            </td>
                            <td>
                                450 * 380
                            </td>
                            <td>
                                Macbook Pro (i7, 2.2GHz, mid 2014)
                            </td>
                            <td>
                                Chrome Version 72.0.3626
                            </td>
                            <td>
                                --
                            </td>
                        </tr>
                    </tbody>
                </table>
                <blockquote>
                    <p>
                        This work is based on the
                        <a href="https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd" rel="nofollow">
                            the coco-ssd tensorflowjs
                        </a>
                        sample. Definitely check it out if you are interested in detecting/tracking any of the 90 classes in the coco dataset.
                    </p>
                </blockquote>
                <p>
                    The library is provided as a useful wrapper to allow you prototype hand/gesture based interactions in your web applications. without the need to understand. It takes in a html image element (
                    <code>
                        img
                    </code>
                    ,
                    <code>
                        video
                    </code>
                    ,
                    <code>
                        canvas
                    </code>
                    elements, for example) and returns an array of bounding boxes, class names and confidence scores.
                </p>
                <p>
                    The library also provides some useful functions (e.g
                    <code>
                        getFPS
                    </code>
                    to get FPS,
                    <code>
                        renderPredictions
                    </code>
                    to draw bounding boxes on a canvas element), and customizable model parameters.
                </p>
                <blockquote>
                    <p>
                        Tests on a Macbook Pro 2.2 GHz Intel Core i7, achieve 21 FPS.
                    </p>
                </blockquote>
                <h2>
                    Community Impact
                </h2>
                <ul>
                    <li>
                        1000 stars on Github
                    </li>
                    <li>
                        <a href="https://victordibia.github.io/handtrack.js/#/" rel="nofollow">
                            Demo
                        </a>
                        viewed over 10k times
                    </li>
                    <li>
                        <a href="https://hackernoon.com/handtrackjs-677c29c1d585" rel="nofollow">
                            Blog Post
                        </a>
                        viewed over 22k times.
                    </li>
                    <li>
                        Over
                        <a href="https://github.com/search?q=handtrack.js&amp;type=Code" rel="nofollow">
                            100 code files
                        </a>
                        on Github (Javascript, HTML) integrate the Handtrack.js library for prototyping.
                    </li>
                    <li>
                        Handtrack.js has been integrated in multiple student projects (including an inprogress application to help children with learning disabilities point and interact with interfaces).
                    </li>
                </ul>
                <h2>
                    How does this work?
                </h2>
                <p>
                    <img data-canonical-url="https://github.com/victordibia/handtrack.js/raw/master/demo/images/architecture.jpg" src="https://res.cloudinary.com/devpost/image/fetch/s--njVY-q8n--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/victordibia/handtrack.js/raw/master/demo/images/architecture.jpg">
                </p>
                <ul>
                    <li>
                        Trained using egohands dataset. You will notice the  model works better when the hands in an image is viewed from a top (egocentic) view.
                    </li>
                    <li>
                        Trained model is converted to the Tensorflowjs format
                    </li>
                    <li>
                        Model is wrapped into an npm package, and can be accessed using
                        <a href="https://www.jsdelivr.com/package/npm/handtrackjs" rel="nofollow">
                            jsdelivr
                        </a>
                        , a free open source cdn that lets you include any npm package in your web application. You may notice the model is loaded slowly the first time the page is opened but gets faster on subsequent loads (caching).
                    </li>
                </ul>
                <h2>
                    When Should I Use Handtrack.js
                </h2>
                <p>
                    <img data-canonical-url="https://github.com/victordibia/handtrack.js/raw/master/demo/images/doodle.gif" src="https://res.cloudinary.com/devpost/image/fetch/s--HeGk72aV--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/victordibia/handtrack.js/raw/master/demo/images/doodle.gif">
                </p>
                <p>
                    If you are interested in prototyping gesture based (body as input) interactive experiences, Handtrack.js can be useful. The usser does not need to attach any additional sensors or hardware but can immediately take advantage of engagement benefits that result from gesture based and body-as-input interactions.
                </p>
                <p>
                    <img data-canonical-url="https://github.com/victordibia/handtrack.js/raw/master/demo/images/pong.gif" src="https://res.cloudinary.com/devpost/image/fetch/s--ckM25eXB--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://github.com/victordibia/handtrack.js/raw/master/demo/images/pong.gif">
                </p>
                <blockquote>
                    <p>
                        Code examples for the pong game control shown above is in the
                        <a href="/demo" rel="nofollow">
                            <code>
                                demo
                            </code>
                            folder
                        </a>
                        .
                    </p>
                </blockquote>
                <p>
                    Some (not all) relevant scenarios are listed below:
                </p>
                <ul>
                    <li>
                        When mouse motion can be mapped to hand motion for control purposes.
                    </li>
                    <li>
                        When an overlap of hand and other objects can represent meaningful interaction signals (e.g a touch or selection event for an object).
                    </li>
                    <li>
                        Scenarios where the human hand motion can be a proxy for activity recognition (e.g. automatically tracking movement activity from a video or images of individuals playing chess). Or simply counting how many humans are present in an image or video frame.
                    </li>
                    <li>
                        You want an accessible demonstration that anyone can easily run or tryout with minimal setup.
                    </li>
                </ul>
                <h2>
                    How Do I Use Handtrack.js in my Web App?
                </h2>
                <h3>
                    via Script Tag
                </h3>
                <p>
                    You can use the library by including it in a javacript script tag.
                </p>
                <pre class="language-markup"><code>
&lt;!-- Load the handtrackjs model. --&gt;
&lt;script src="https://cdn.jsdelivr.net/npm/handtrackjs/dist/handtrack.min.js"&gt; &lt;/script&gt;

&lt;!-- Replace this with your image. Make sure CORS settings allow reading the image! --&gt;
&lt;img id="img" src="hand.jpg"/&gt; 
&lt;canvas id="canvas" class="border"&gt;&lt;/canvas&gt;

&lt;!-- Place your code in the script tag below. You can also use an external .js file --&gt;
&lt;script&gt;
// Notice there is no 'import' statement. 'handTrack' and 'tf' is
// available on the index-page because of the script tag above.

const img = document.getElementById('img'); 
const canvas = document.getElementById('canvas');
const context = canvas.getContext('2d');

// Load the model.
handTrack.load().then(model =&gt; {
// detect objects in the image.
model.detect(img).then(predictions =&gt; {
console.log('Predictions: ', predictions); 
});
});
&lt;/script&gt;
</code></pre>
                <h3>
                    via NPM
                </h3>
                <pre class="language-shell"><code>npm install --save handtrackjs
</code></pre>
                <pre class="language-javascript"><code>import * as handTrack from 'handtrackjs';

const img = document.getElementById('img');

// Load the model.
handTrack.load().then(model =&gt; {
// detect objects in the image.
console.log("model loaded")
model.detect(img).then(predictions =&gt; {
console.log('Predictions: ', predictions); 
});
});
</code></pre>
                <h2>
                    API
                </h2>
                <h4>
                    Loading the model: handTrack.load()
                </h4>
                <p>
                    Once you include the js module, it is available as
                    <code>
                        handTrack
                    </code>
                    . You can then load a model with optional parameters.
                </p>
                <pre class="language-javascript"><code>
const modelParams = {
flipHorizontal: true,   // flip e.g for video 
imageScaleFactor: 0.7,  // reduce input image size for gains in speed.
maxNumBoxes: 20,        // maximum number of boxes to detect
iouThreshold: 0.5,      // ioU threshold for non-max suppression
scoreThreshold: 0.79,    // confidence threshold for predictions.
}

handTrack.load(modelParams).then(model =&gt; {

});

</code></pre>
                <p>
                    Returns a
                    <code>
                        model
                    </code>
                    object.
                </p>
                <h4>
                    Detecting hands: model.detect()
                </h4>
                <p>
                    <code>
                        model.detect
                    </code>
                    takes an input image element (can be an
                    <code>
                        img
                    </code>
                    ,
                    <code>
                        video
                    </code>
                    ,
                    <code>
                        canvas
                    </code>
                    tag) and returns an array of bounding boxes with class name and confidence level.
                </p>
                <pre class="language-javascript"><code>model.detect(img).then(predictions =&gt; { 

});
</code></pre>
                <p>
                    Returns an array of classes and confidence scores that looks like:
                </p>
                <pre class="language-javascript"><code>[{
        bbox: [x, y, width, height],
  class: "hand",
  score: 0.8380282521247864
}, {
bbox: [x, y, width, height],
class: "hand",
score: 0.74644153267145157
}]
</code></pre>
                <h4>
                    Other Helper Methods
                </h4>
                <ul>
                    <li>
                        <code>
                            model.getFPS()
                        </code>
                        : get FPS calculated as number of detections per second.
                    </li>
                    <li>
                        <code>
                            model.renderPredictions(predictions, canvas, context, mediasource)
                        </code>
                        :  draw bounding box (and the input mediasource image) on the specified canvas.
                        <code>
                            predictions
                        </code>
                        are an array of results from the
                        <code>
                            detect()
                        </code>
                        method.
                        <code>
                            canvas
                        </code>
                        is a reference to a html canvas object where the predictions should be rendered,
                        <code>
                            context
                        </code>
                        is the canvas 2D context object,
                        <code>
                            mediasource
                        </code>
                        a reference to the input frame (img, video, canvas etc) used in the prediction (it is first rendered, and the bounding boxes drawn on top of it).
                    </li>
                    <li>
                        <code>
                            model.getModelParameters()
                        </code>
                        : returns model parameters.
                    </li>
                    <li>
                        <code>
                            model.setModelParameters(modelParams)
                        </code>
                        : updates model parameters with
                        <a href="#loading-the-model-handtrackload" rel="nofollow">
                            <code>
                                modelParams
                            </code>
                        </a>
                    </li>
                    <li>
                        <code>
                            dispose()
                        </code>
                        : delete model instance
                    </li>
                    <li>
                        <code>
                            startVideo(video)
                        </code>
                        : start webcam video stream on given video element. Returns a promise that can be used to validate if user provided video permission.
                    </li>
                    <li>
                        <code>
                            stopVideo(video)
                        </code>
                        : stop video stream.
                    </li>
                </ul>
                <h2>
                    Run Demo
                </h2>
                <p>
                    Commands below runs the demo example in the
                    <code>
                        demo
                    </code>
                    folder.
                </p>
                <p>
                    <code>
                        npm install
                    </code>
                    <code>
                        npm run start
                    </code>
                </p>
                <p>
                    The start script launches a simple
                    <code>
                        python3
                    </code>
                    webserver from the demo folder using
                    <code>
                        http.server
                    </code>
                    . You should be able to view it in your browser at
                    <a href="http://localhost:3005/" rel="nofollow">
                        http://localhost:3005/
                    </a>
                    . You can also view the pong game control demo on same link
                    <a href="http://localhost:3005/pong.html" rel="nofollow">
                        http://localhost:3005/pong.html
                    </a>
                </p>
                <h2>
                    How was this built?
                </h2>
                <p>
                    The object detection model used in this project was trained using annotated images of the human hand (
                    <a href="https://github.com/victordibia/handtracking/issues" rel="nofollow">
                        see here
                    </a>
                    ) and converted to the tensorflow.js format. This wrapper library was created using guidelines and some code adapted from
                    <a href="https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd" rel="nofollow">
                        the coco-ssd tensorflowjs
                    </a>
                    .
                </p>
                <h2>
                    TODO (ideas welcome)
                </h2>
                <ul>
                    <li>
                        This thing is still compute heavy (your fans may spin after while). This is mainly because of the neural net operations needed to predict bounding boxes. Perhaps there might be ways to improve/optimize this.
                    </li>
                    <li>
                        Tracking id's across frames. Perhaps some nifty methods that assigns ids to each had as they enter a frame and tracks them (e.g based on naive euclidean distance).
                    </li>
                    <li>
                        Add some discrete poses (e.g. instead of just hand, detect open hand, fist).
                    </li>
                </ul>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag">
                            tensorflow
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            tensorflowjs
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://victordibia.github.io/handtrack.js/#/" rel="nofollow" target="_blank" title="https://victordibia.github.io/handtrack.js/#/">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                victordibia.github.io
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
