<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div>
                <p>
                    Imagine a framework-agnostic DSL with strong typing, dimension and ownership checking and lots of syntax sugar. What would it be like? As interesting as it is, here is why there needs to a langauge for neural network:
                </p>
                <ol>
                    <li>
                        <p>
                            Multi-target. Write once, run everywhere(interpreted in PyTorch or compiled to Tensorflow MetaGraphDef).
                        </p>
                    </li>
                    <li>
                        <p>
                            Typechecking with good type annotation.
                        </p>
                    </li>
                    <li>
                        <p>
                            Parallellize with language-level directives
                        </p>
                    </li>
                    <li>
                        <p>
                            Composition(constructing larger systems with building blocks) is easier
                        </p>
                    </li>
                    <li>
                        <p>
                            Ownership system because GC in CUDA is a nightmare
                        </p>
                    </li>
                    <li>
                        <p>
                            No more frustration from deciphering undocumented code written by researchers. The issue is, an overwhelming majority of researchers are not programmers, who care about the aesthetics of clean code and helpful documentation. I get it - people are lazy and unsafe unless the compiler forces them to annotate their code.
                        </p>
                    </li>
                </ol>
                <p>
                    About syntax:
                </p>
                <ol>
                    <li>
                        <p>
                            Stateful symbols are capitalized
                        </p>
                    </li>
                    <li>
                        <p>
                            Impure functions must be annotated
                        </p>
                    </li>
                    <li>
                        <p>
                            Local variables cannot be shadowed
                        </p>
                    </li>
                    <li>
                        <p>
                            Function args have keyword
                        </p>
                    </li>
                </ol>
                <pre class="language-rust"><code>use conv::{Conv2d, Dropout2d, maxpool2d};
use nonlin::relu;
use lin::Linear;

// three levels of abstraction: node, weights, graph
// ? stands for batch size, channel, height, width
// outputs tensor of shape [batch_size, 10]
node Mnist&lt;?,c,h,w -&gt; ?,OUT&gt; {
// define symbol level macro for both type/dimension and scope variable
FC1 = 320;
FC2 = 50;
OUT = 10;
}
weights Mnist&lt;?,c,h,w -&gt; ?,OUT&gt; {
conv1 = Conv2d::&lt;?,c,h,w -&gt; ?,c,h,w&gt;::new(in_ch=1, out_ch=10, kernel_size=5);
conv2 = Conv2d::&lt;?,c,h,w -&gt; ?,c,h,w&gt;::new(in_ch=10, out_ch=20, kernel_size=5);
dropout = Dropout2d::&lt;?,c,h,w -&gt; ?,c,h,w&gt;::new(p=0.5);
fc1 = Linear::&lt;?,FC1 -&gt; ?,FC2&gt;::new();
fc2 = Linear::&lt;?,FC2 -&gt; ?,OUT&gt;::new();
}
graph Mnist&lt;?,c,h,w -&gt; ?,OUT&gt; {
fn new() {
fc1.init_normal(std=1.);
fc2.init_normal(std=1.);
}

fn forward(self, x) {
x
|&gt; conv1            |&gt; maxpool2d(kernel_size=2)
|&gt; conv2 |&gt; dropout |&gt; maxpool2d(kernel_size=2)
|&gt; view(?, FC1)
|&gt; fc1 |&gt; relu
|&gt; self.fc2()
|&gt; log_softmax(dim=1)
}

fn fc2(self, x: &lt;?,FC2&gt;) -&gt; &lt;?,OUT&gt; {
x |&gt; fc2 |&gt; relu
}
}
</code></pre>
                <p>
                    compiles to
                </p>
                <pre class="language-python"><code>from torch import nn, F

class Mnist(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
self.conv2_drop = nn.Dropout2d()
self.fc1 = nn.Linear(320, 50)
self.fc2 = nn.Linear(50, 10)

def forward(self, x):
x = F.relu(F.max_pool2d(self.conv1(x), 2))
x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
x = x.view(-1, 320)
x = F.relu(self.fc1(x))
x = F.dropout(x, training=self.training)
x = self.fc2(x)
return F.log_softmax(x, dim=1)
</code></pre>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/pytorch">
                                pytorch
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            rust
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/rickyhan/tensorscript-compiler" rel="nofollow" target="_blank" title="https://github.com/rickyhan/tensorscript-compiler">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
