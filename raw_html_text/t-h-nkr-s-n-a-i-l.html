<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li>
                        <div class="flex-video">
                            <iframe allowfullscreen="allowfullscreen" allowscriptaccess="always" class="video-embed" frameborder="0" height="371" mode="transparent" src="https://www.youtube.com/embed/bwTws_g2Cx4?enablejsapi=1&amp;hl=en_US&amp;rel=0&amp;start=&amp;version=3&amp;wmode=transparent" type="text/html" webkitallowfullscreen="true" width="660" wmode="transparent">
                            </iframe>
                        </div>
                    </li>
                </ul>
            </div>
            <div>
                <h2>
                    Inspiration
                </h2>
                <p>
                    T(H)NKR was originally intended to be a simple object detection interface that would enable users to shop for items online by simply taking pictures of them. However, we realized that there are many companies out there such as Google, with it's Google Lenses and Google Goggles, and Samsung with Bixby vision.
                </p>
                <p>
                    We realized that our former project idea was no different from what was already available to consumers.
                    <strong>
                        We wanted T(H)NKR to be a unified solution to all object-detection-based e-commerce applications, not only for the hale and hearty but for the special needs community as well! To solve a problem that has been viewed as a  challenging feat for many, we envisioned the need for a universal application that is suitable for all demographics, hence leading to the invention of the Special Needs Artificial Intelligence Library or S.N.A.I.L. for short.
                    </strong>
                </p>
                <h2>
                    The present
                </h2>
                <p>
                    Today, applications that claim to be helping the special needs community have been successful in limited ways. We hope to amplify the effect by broadening the target audience spectrum. By engaging those with special needs, as well as those,  who are able, we aim to make life a bit easier for both demographics. Special needs apps today are mostly restricted to one disability. At times, inaccuracy of commonly-used Machine Learning algorithms has led to the invention of more applications and software that boast high results.
                </p>
                <h2>
                    What it does
                </h2>
                <p>
                    The T(H)NKR can scan and identify consumer products such as shoes, bottles, and apparel. Using custom Object Detection algorithms, the app draws their attention to online stores that sell similar products at cheaper prices. The app facilitates communication between the abled and the less-abled to create a unique experience.
                </p>
                <p>
                    Sign Language is one of the most important methods of communication for the deaf and mute today. The T(H)NKR S.N.A.I.L. eases the communication boundary between the abled and the less-abled. It features a custom-built Sign Language to Speech/Text translation engine that processes the positions of the hand in real time. Words are displayed on a screen to verbalize the wearer's thoughts.
                </p>
                <p>
                    For the blind, we use object detection to identify obstacles in their path and the proximity to possible dangers.  Common examples include road-crossings, where the application can alert the wearer about oncoming vehicles and traffic, thereby minimising the risks of accidents. Another common use case would be the presence of an elevation or demotion along their path. By identifying the obstacle before-hand, an alert is sent out which draws closer cautiousness towards the situation.
                </p>
                <p>
                    We envision the app to be part of a whole suite of multi-demographic applications available for download on smartphones.
                </p>
                <h2>
                    How we built it
                </h2>
                <p>
                    The backbone of the application is an
                    <strong>
                        Object Detection engine
                    </strong>
                    that is able to predict the presence of common day-to-day items. With an easy-to-use user interface, coupled with the straightforward functionality, the application sets itself apart from most applications that are aimed at aiding the less-abled.
                </p>
                <p>
                    The Machine Learning libraries involve a bare python script that runs a TensorFlow Computational Graph that takes in raw pixel values of its surroundings via webcam and predicts the probability of daily items existing in each state or frame. Functioning at 20fps, the real-time translation engine is quick in its predictions and is able to give accurate outputs irrespective of the lighting condition in its environment.
                </p>
                <p>
                    The user interface was designed and brought to life using Adobe Experience designer, a desktop app that is commonly used to design web and mobile applications through active wireframing and quick prototyping.
                </p>
                <p>
                    The Speech to Text engine uses the Google Voice Recognition API as training our own voice recognition machine learning model would be too computationally expensive and costly in terms of time.
                </p>
                <h2>
                    Challenges we ran into
                </h2>
                <p>
                    The problems faced while making the app were seen throughout the hackathon resulting in the absence of an actual working prototype. The Mchine Learning model we built, though only 200 MiB, was too large to be uploaded to Cloud GPU clusters on many commonly-used platforms such as Floydhub, Azure ML, Google Cloud and Paperspace. Our failure in submission lies in the services we chose to use and the problems that arose due to the inability of the model to be fully trained and tested during the development phase.
                </p>
                <h2>
                    Accomplishments that we're proud of
                </h2>
                <p>
                    Thought the odds were against us in terms of hardware provisioning and service dysfunctionality, we are amused at the tireless hours we put in in order to get the app up and running. Despite aimlessly running around for help, we were able to get most of the training done, though the full training and testing cycle was left incomplete.
                </p>
                <p>
                    We were able to reproduce our vision for the application using a brief wireframe that we created during the final moments of the hackathon.
                </p>
                <h2>
                    What we learned
                </h2>
                <p>
                    Despite weeks of planning and testing of new ideas, at times, some things are not meant to work out. We learned this the hard way by failing to meet the requirements for the finals-selection. Many arguments followed by hours of careful, tedious planning were met with serious competition from the other participating teams.
This is just the beginning of what could potentially be a great application of Machine Learning in a real-world use-case.
                </p>
                <p>
                    The team has actively been learning new technologies that required them to use a different methodology of thinking.
                </p>
                <p>
                    We have met a lot of people across the span of two days. We have learned as much from them as they from us. We hope that this hackathon experience has brought out the more intuitive side of the participants!
                </p>
                <h2>
                    T(H)NKR in the years to come...
                </h2>
                <p>
                    As of now, we aim to complete the training of the application ML model. We hope to procure high accuracy from the model and we wish to see it come to fruition. There is still a long way to go and many exciting updates to follow. This project has taught us what it feels like to collaborate and share knowledge with one another. 
We hope to release the application to the public so as to better benefit the community - especially those who are less fortunate than most of us.
                </p>
                <p>
                    Regards,
    Team T(H)NKR
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag">
                            adobe-xd
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            floydhub
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/google-cloud">
                                google-cloud
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            paperspace
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            tensorflow
                        </span>
                    </li>
                </ul>
            </div>
        </div>
    </body>
</html>
