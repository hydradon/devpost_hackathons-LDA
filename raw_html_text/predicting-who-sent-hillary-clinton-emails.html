<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div>
                <p>
                    I wanted to play with some data so I downloaded Hillary's emails from
                    <a rel="nofollow">
                        Kaggle
                    </a>
                    . I also wanted to play with
                    <a href="https://github.com/spotify/annoy" rel="nofollow">
                        Spotify's Annoy
                    </a>
                    .
                </p>
                <p>
                    I formulated the problem of predicting who emailed Hillary using a machine learning framework. I filtered the emails by those that were sent to "H", a codename for Hillary. I then took all the emails from the 15 people that emailed her the most. I used the body of the email as the features and I attempted to predict the name of the sender. A random guess would be 1/15 or about 7%.
                </p>
                <p>
                    I represented the textual data using
                    <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="nofollow">
                        tf-idf
                    </a>
                    , one of the most popular ways to represent text. I then used three different classifiers to predict who sent each email. The classifiers I used were:
                </p>
                <ol>
                    <li>
                        Naive Bayes
                    </li>
                    <li>
                        Stochastic Gradient Descent Classifier
                    </li>
                    <li>
                        k-Nearest Neighbors - implemented with Annoy (Appropriate nearest neighbors oh yeah!)
                    </li>
                </ol>
                <p>
                    I was able to achieve over 60% accuracy, which is much better than a random guess, however I did run into some challenges.
                </p>
                <ul>
                    <li>
                        <p>
                            high-dimensionality
                        </p>
                        <ul>
                            <li>
                                although I was just using text information as the features, I still had over 25k features (even after removing common words like "like", "the", and "and"
                            </li>
                            <li>
                                to combat this, I used
                                <a href="https://en.wikipedia.org/wiki/Random_projection" rel="nofollow">
                                    Random Projections
                                </a>
                                , which not only reduced the time it took to train and predict, but also in some cases improved the accuracy
                            </li>
                        </ul>
                    </li>
                    <li>
                        <p>
                            unbalanced dataset
                        </p>
                        <ul>
                            <li>
                                there were 3 people that emailed "H" more than everyone else combined. This is a problem as classifiers tend to learn that only these 3 people are important and ignore the rest of the classes (or people). Although this can be advantageous, I wanted to try to learn each class equally
                            </li>
                            <li>
                                to combat this, I divided the dataset into a balanced dataset. This hurt the accuracy across all classifiers but it tells a more telling story.
                            </li>
                        </ul>
                    </li>
                </ul>
                <p>
                    Overall, Stochastic gradient descent (sgd) was the faster and more accurate classifier using all the tf-idf features, k-nearest neighbors was more accurate but a bit slower than sgd for the reduced dimensions dataset, and the knn did the best in terms of speed and accuracy for the balanced dataset.
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/pandas">
                                pandas
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            sklearn
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://gist.github.com/benlawson/fd40a5b779b11f24f4d1c811214857a4" rel="nofollow" target="_blank" title="https://gist.github.com/benlawson/fd40a5b779b11f24f4d1c811214857a4">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                gist.github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
