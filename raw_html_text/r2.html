<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div>
                <h2>
                    Inspiration
                </h2>
                <p>
                    Trying to use vision and human body language recognition to drive the movements/responses or Roboy.
We say that body language tells us more about a person than their words, and we wanted to enhance Roboy in this way in his interactions.
                </p>
                <h2>
                    What it does
                </h2>
                <p>
                    Uses Microsoft Kinect to sense and define human gestures, which then trigger the expected human reaction from another human. For example, if the human waves, Roboy waves back - if the human puts out their hand to shake hands, Roboy should reach out to shake that hand!
                </p>
                <h2>
                    How I built it
                </h2>
                <p>
                    All systems (recording surroundings and sending commands to Roboy) are all based on ROS.
                </p>
                <h2>
                    Challenges I ran into
                </h2>
                <p>
                    Calibrating the commands to send to Roboy so that the movements created were meaningful. The system of pulleys is quite complicated, so producing something as 'simple' as a handshake was a real challenge!
                </p>
                <h2>
                    Accomplishments that I'm proud of
                </h2>
                <p>
                    Getting the complete pipeline functional, from running the Kinect and recognising human gestures to sending them through a well-built control system that then manages commands between connected systems.
                </p>
                <h2>
                    What I learned
                </h2>
                <p>
                    Interactions between ROS and Roboy
Mechanics of a real robot based on human muscle systems
                </p>
                <h2>
                    What's next for R2
                </h2>
                <p>
                    The first thing would be to fine-tune the physical movements that Roboy can make.
After that it would be great to include some emotion into our motion - perhaps combining speech output with Roboy's gesture to create a more complete interaction. For example, if someone approaches and attempts to handshake or hug and is also smiling, Roboy would react with the appropriate gesture and also say how happy he is to see you, or ask how you are. This could be implemented using Affectiva.
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/c--3">
                                c++
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            microsoft-kinect
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            ros
                        </span>
                    </li>
                </ul>
            </div>
        </div>
    </body>
</html>
