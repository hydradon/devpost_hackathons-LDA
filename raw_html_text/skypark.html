<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li>
                        <div class="flex-video">
                            <iframe allowfullscreen="allowfullscreen" allowscriptaccess="always" class="video-embed" frameborder="0" height="371" mode="transparent" src="https://www.youtube.com/embed/ojqvGgKvx7c?enablejsapi=1&amp;hl=en_US&amp;rel=0&amp;start=&amp;version=3&amp;wmode=transparent" type="text/html" webkitallowfullscreen="true" width="660" wmode="transparent">
                            </iframe>
                        </div>
                    </li>
                </ul>
            </div>
            <div>
                <h2>
                    Inspiration
                </h2>
                <p>
                    Prior to the sponsor presentation at the opening ceremony, our team went in with the idea that we were limited to webdev and/or appdev. During the sponsor presentation, when the words "Raspberry Pi 4" were spoken, my teammate and I looked at each other and we didn't even say a word, and we knew what we wanted to do instead. Next thing we knew, we attended the workshop and were so excited to see computer vision in person, and borrowed a Pi 4 from the alwaysAI table.
                </p>
                <h2>
                    What it does
                </h2>
                <p>
                    Our program uses real-time object detection to detect cars and evaluate the number of parking spaces available (or not available) from a video livestream via the Pi 4 camera.
                </p>
                <h2>
                    How we built it
                </h2>
                <ol>
                    <li>
                        <strong>
                            <em>
                                Reading documentation
                            </em>
                        </strong>
                        : we read the documentation as well as we could to try and understand it to the best of our ability and follow along with the set-up guide
                    </li>
                    <li>
                        <strong>
                            <em>
                                Collecting relevant data
                            </em>
                        </strong>
                        : after the setup, we had to collect only the relevant data, such as only detecting cars instead of other objects within the pre-trained model.
                    </li>
                    <li>
                        <strong>
                            <em>
                                Manipulating data to our vertical
                            </em>
                        </strong>
                        : our solution for the sustainability vertical was to tackle the traffic/parking problem within LA. We had to devise a formula, given that we know the width of the cars, calculate the number of available spots in between or evaluate that there were no spots in between.
                    </li>
                    <li>
                        <strong>
                            <em>
                                Testing, testing, testing
                            </em>
                        </strong>
                        : we tested and tested along the way with each step we progressed. First, we were attempting to detect anything from the objects of the trained model. Then, we were testing to see if the program could only detect images of cars, and ignore everything else. Finally, we moved onto testing the program against images of several parked cars.
                    </li>
                    <li>
                        <strong>
                            <em>
                                Not being afraid to ask for help
                            </em>
                        </strong>
                        : A lot of smart people are at the hackathon, and our team had to put any pride or ego aside and not be afraid to ask for help, even for the most trivial matter. We especially want to thank alwaysAI's Taiga, who helped us tremendously, and with the kindest attitude!
                    </li>
                </ol>
                <h2>
                    Challenges we ran into
                </h2>
                <ol>
                    <li>
                        Could not ssh into the Pi 4 without an ethernet cord. So we had to wait until the next day to be even able to ssh into the Pi 4.
                    </li>
                    <li>
                        Most of our members only knew basic python and had never worked with computer vision API before.
                    </li>
                    <li>
                        General hardware and software setup had a steep learning curve for a beginner.
                    </li>
                </ol>
                <h2>
                    Accomplishments that we're proud of
                </h2>
                <p>
                    We have never worked with coding hardware, and especially we were all fresh beginners to computer vision. The fact that we could even detect an object, let alone use the properties of the objects from the alwaysAI API to calculate a result, we are very proud! 
We faced a lot of problems with the setup in the beginning, and in the middle stages, we were close to abandoning the project. Although we were super enthusiastic, we weren't sure if we had the skills to be able to demo. 
However, we pushed through, and put our minds together and took advantage of each other's skills to be able to code our program!
                </p>
                <h2>
                    What we learned
                </h2>
                <p>
                    Our members learned a lot about python, as it was not a language they used for coding hardware prior to the hackathon. We learned a lot about using APIs, and of course, how
                    <strong>
                        <em>
                            cool
                        </em>
                    </strong>
                    computer vision is. There are so many options for models to use for applications that can benefit the real world.
                </p>
                <h2>
                    What's next for Skypark
                </h2>
                <p>
                    If Skypark can take a live videostream from a camera that is more fitted to observing an entire parking lot, and working with that bigger dataset, that would be fantastic progress for Skypark! Ultimately, we built the project to benefit the (in)famous car problem of LA, We see that our program can be further developed to accommodate further real-life concerns and parameters so that it can be best optimized for everyday use.
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag">
                            alwaysai
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            edge-iq
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/raspberry-pi">
                                raspberry-pi
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
        </div>
    </body>
</html>
