<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div>
                <h1>
                    Okay, what's going on here?
                </h1>
                <p>
                    As the world moves towards a larger amount of smaller and smaller devices, we need to make sure that "non-human" data throughput is kept to a possible minimum. Continuous software updates and best-case linear increases in the number of these devices lead to quadratic data throughput growth, which simply cannot be answered with current infrastructural solutions.
                </p>
                <blockquote>
                    <p>
                        ever tried to
                        <code>
                            git clone torvalds/linux@master
                        </code>
                        ?
                    </p>
                </blockquote>
                <p>
                    On developers' sides, version control systems like
                    <code>
                        git
                    </code>
                    have had a large impact in aiding collaboration as a whole. However, most VCSes are full of overhead. The "line-change" delta approach as well as the uncompressed nature of data within git is problematic. Within any VCS, old data is at rest by definition. Epsilon could help by strongly deflating this data, which would lead to considerable cost benefits during transfers. (Large companies in the VCS-as-a-service field have rather strong influence and would likely be successful in getting the subsystem implemented in
                    <code>
                        git
                    </code>
                    and co.)
                </p>
                <h1>
                    What can we do about it?
                </h1>
                <p>
                    Epsilon's constant-memory design means that even low-memory devices can implement it with little overhead. Decoding is a matter of repeated memory copying, and dual (backwards/forwards) tables can be used to compress duplicate-rich data at rest: Wearable devices which have to display a stream of notifications could be pushed such a format by their phones, thereby getting the ability to retain more data over a longer time.
                </p>
                <p>
                    VCSes are a particularly interesting use for the format because Epsilon-encoding
                    <strong>
                        strict subsets
                    </strong>
                    of a given file is extremely efficient&mdash;both in space and time.
                </p>
                <h1>
                    How does Epsilon work?
                </h1>
                <p>
                    At its core, Epsilon is defined as a binary format with few restrictions on the nature of its data. (One exception: the current C implementation does not support null characters due to stdlib limitations.)
The one limitation is that (as with any diff algorithm) the entire "corpus", or the base data file, must be accessible (in-memory) for en- and decoding. 
The interesting parts of Epsilon begin at this in-memory corpus model: it becomes very simple to look up certain phrases, meaning that the corpus can itself be used as a dictionary (effectively causing the diff process to lead to a minor kind of compression.)
                </p>
                <h1>
                    What data structure does the protocol use?
                </h1>
                <p>
                    (Further optimization of these fields is possible. In the current state, only files up to 4 gigabytes are supported.)
                </p>
                <p>
                    The exported/imported data structure is a blob composed of self-describing fields. (Strings have their lengths predefined to facilitate quick
                    <code>
                        memcpy
                    </code>
                    .)
                </p>
                <p>
                    In the current reference implementation, there are only two kinds of such fields:
                    <code>
                        lookup
                    </code>
                    and
                    <code>
                        literal
                    </code>
                    .
For the
                    <code>
                        literal
                    </code>
                    field type, the overhead ranges from 2 to 5 bytes depending on the data length.
                    <code>
                        lookup
                    </code>
                    entries take between 3 and 9 bytes of space, depending on the size of the corpus.
                </p>
                <p>
                    Attempts to use direct tideover (in effect, keeping a pointer to the corpus and looking at what is retained through a change-detection mechanism) had a strong negative performance impact.
                </p>
                <p>
                    Future improvements will have to be mostly on the encoder's side (with the exception of retained insertions, see below under optimizations.)
                </p>
                <h1>
                    How good is Epsilon at its job?
                </h1>
                <p>
                    The Python and C reference implementations have compatible command-line APIs and can be configured to have exactly equivalent behavior.
The "default" C implementation and the C hashtable implementation achieve the following speed results on the data:
                </p>
                <table class="responsive">
                    <thead>
                        <tr>
                            <th>
                                File A size (KiB)
                            </th>
                            <th>
                                File B size (KiB)
                            </th>
                            <th>
                                Diff size (KiB)
                            </th>
                            <th>
                                Scramble type
                            </th>
                            <th>
                                Time to encode (Normal, in milliseconds)
                            </th>
                            <th>
                                Time to encode (with hashtable, in milliseconds)
                            </th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>
                                72.70
                            </td>
                            <td>
                                64.38
                            </td>
                            <td>
                                7.82
                            </td>
                            <td>
                                Sentences,Words
                            </td>
                            <td>
                                130
                            </td>
                            <td>
                                60 (1/2)
                            </td>
                        </tr>
                        <tr>
                            <td>
                                4.23
                            </td>
                            <td>
                                3.49
                            </td>
                            <td>
                                0.307
                            </td>
                            <td>
                                Few Sentences
                            </td>
                            <td>
                                &lt;10
                            </td>
                            <td>
                                &lt;10 (unknown)
                            </td>
                        </tr>
                        <tr>
                            <td>
                                274.6
                            </td>
                            <td>
                                244.1
                            </td>
                            <td>
                                49.83
                            </td>
                            <td>
                                Mixed
                            </td>
                            <td>
                                2300
                            </td>
                            <td>
                                160 (1/14)
                            </td>
                        </tr>
                    </tbody>
                </table>
                <p>
                    The hashtable-based implementation generates slightly larger diff files (by around 5%.)
                </p>
                <p>
                    The hash table implementation allows reuse of the same key (as well as entry&rarr;next to get items with such keys) and is using a table size of approximately 11000, (a prime to minimize collisions)
                </p>
                <h2>
                    What further optimization is possible?
                </h2>
                <p>
                    Though good size-reduction results were achieved with the corpus-search strategy, there's certainly room for optimization.
                </p>
                <h3>
                    To improve both encoding and decoding:
                </h3>
                <ul>
                    <li>
                        Unlike repeated insertions of text
                        <em>
                            which is found in the corpus
                        </em>
                        , repeated additions are currently not optimized for. Ideally, en-/decoders would take keep track of these.
                    </li>
                    <li>
                        Multithreaded encoding is hypothetically possible (and no locks would be needed due to staticity of the corpus), but might cause minor increases in file size as otherwise extraneous commands might be included.
                    </li>
                    <li>
                        (Further platform-specific optimizations would also be helpful.)
                    </li>
                </ul>
                <h3>
                    To improve encoding:
                </h3>
                <ul>
                    <li>
                        It'd be interesting to adapt the hash table's properties to data as it processes. For example, if longer runs are detected, the hash table could be recomputed to take into account a later index (rebuilding the table and changing the key function), thereby decreasing false-positive rates.
                    </li>
                    <li>
                        Predicting possible match sites (for example, using stochastic models) could further improve efficiency, but I frankly have no expertise in the field :)
                    </li>
                </ul>
                <h1>
                    Verifying results
                </h1>
                <p>
                    The following is the verbatim output of
                    <code>
                        make test
                    </code>
                    on a Mac (High Sierra) with dependencies installed. If further verification is required, please talk to Johannes.
                </p>
                <pre class="language-nolang"><code>gcc epsilon.c -O3 -o epsilon.out
gcc epsilon-hash-map.c -O3 -o epsilon-hash-map.out
rm *.tmp || true
rm: *.tmp: No such file or directory
./epsilon.py test 1_in 1_out
1_in  72.70 Ki
1_out  64.38 Ki
100.0%  &epsilon;  Produced  1506 records
100.0%  &epsilon;  Exported  7.82 Ki
[ OK ]  &epsilon;  Imported 
[PASS]

./epsilon.py test 2_in 2_out
2_in  4.23 Ki
2_out  3.49 Ki
100.0%  &epsilon;  Produced  62 records
100.0%  &epsilon;  Exported  315 B
[ OK ]  &epsilon;  Imported 
[PASS]

./epsilon.py test 3_in 3_out
3_in  274.59 Ki
3_out  244.09 Ki
100.0%  &epsilon;  Produced  6313 records
100.0%  &epsilon;  Exported  49.83 Ki
[ OK ]  &epsilon;  Imported 
[PASS]

./epsilon.py diff 1_in 1_out 1_diff.from-python.tmp
./epsilon.py diff 2_in 2_out 2_diff.from-python.tmp
./epsilon.py diff 3_in 3_out 3_diff.from-python.tmp
time ./epsilon.out diff 1_in 1_out 1_diff.from-c.tmp
0.15 real         0.14 user         0.00 sys
./epsilon.out apply 1_in 1_diff.from-c.tmp 1_out.from-c.tmp; echo '\n\n'



shasum 1_diff.from-python.tmp 1_diff.from-c.tmp; echo
afafebe5e0a7f87235529723ffce10f2ac0d4194  1_diff.from-python.tmp
afafebe5e0a7f87235529723ffce10f2ac0d4194  1_diff.from-c.tmp

shasum 1_out 1_out.from-c.tmp; echo
80b438320d9e2d24f7c83bed3cc8714fdcb937df  1_out
80b438320d9e2d24f7c83bed3cc8714fdcb937df  1_out.from-c.tmp

time ./epsilon.out diff 2_in 2_out 2_diff.from-c.tmp
0.00 real         0.00 user         0.00 sys
./epsilon.out apply 2_in 2_diff.from-c.tmp 2_out.from-c.tmp; echo '\n\n'



shasum 2_diff.from-python.tmp 2_diff.from-c.tmp; echo
084ad7719dc4209884cfc6306a3d56ac36c6d7b7  2_diff.from-python.tmp
084ad7719dc4209884cfc6306a3d56ac36c6d7b7  2_diff.from-c.tmp

shasum 2_out 2_out.from-c.tmp; echo
1df60227dd7b949493a54c13a03812875c3ba7d1  2_out
1df60227dd7b949493a54c13a03812875c3ba7d1  2_out.from-c.tmp

time ./epsilon.out diff 3_in 3_out 3_diff.from-c.tmp
2.31 real         2.27 user         0.01 sys
./epsilon.out apply 3_in 3_diff.from-c.tmp 3_out.from-c.tmp; echo

shasum 3_diff.from-python.tmp 3_diff.from-c.tmp; echo
18bce9d051b9ce310a91c56b55e38bb9ad10e760  3_diff.from-python.tmp
18bce9d051b9ce310a91c56b55e38bb9ad10e760  3_diff.from-c.tmp

shasum 3_out 3_out.from-c.tmp; echo
b43955f37c2e3ac90c0cc2f3ce9b515d16087c68  3_out
b43955f37c2e3ac90c0cc2f3ce9b515d16087c68  3_out.from-c.tmp

time ./epsilon-hash-map.out diff 1_in 1_out 1_diff.hashmap.tmp
0.04 real         0.02 user         0.00 sys
./epsilon-hash-map.out apply 1_in 1_diff.hashmap.tmp 1_out.hashmap.tmp
echo; wc 1_diff.hashmap.tmp; echo "chars in diff for hashmap on file 1"

171     856    8534 1_diff.hashmap.tmp
chars in diff for hashmap on file 1
shasum 1_out 1_out.hashmap.tmp; echo
80b438320d9e2d24f7c83bed3cc8714fdcb937df  1_out
80b438320d9e2d24f7c83bed3cc8714fdcb937df  1_out.hashmap.tmp

time ./epsilon-hash-map.out diff 2_in 2_out 2_diff.hashmap.tmp
0.00 real         0.00 user         0.00 sys
./epsilon-hash-map.out apply 2_in 2_diff.hashmap.tmp 2_out.hashmap.tmp
echo; wc 2_diff.hashmap.tmp; echo "chars in diff for hashmap on file 2"

13      59     325 2_diff.hashmap.tmp
chars in diff for hashmap on file 2
shasum 2_out 2_out.hashmap.tmp; echo
1df60227dd7b949493a54c13a03812875c3ba7d1  2_out
1df60227dd7b949493a54c13a03812875c3ba7d1  2_out.hashmap.tmp

time ./epsilon-hash-map.out diff 3_in 3_out 3_diff.hashmap.tmp
0.17 real         0.14 user         0.01 sys
./epsilon-hash-map.out apply 3_in 3_diff.hashmap.tmp 3_out.hashmap.tmp
echo; wc 3_diff.hashmap.tmp; echo "chars in diff for hashmap on file 3"

1340    5763   53473 3_diff.hashmap.tmp
chars in diff for hashmap on file 3
shasum 3_out 3_out.hashmap.tmp; echo
b43955f37c2e3ac90c0cc2f3ce9b515d16087c68  3_out
b43955f37c2e3ac90c0cc2f3ce9b515d16087c68  3_out.hashmap.tmp

####### If all went well, all couples of shasums above are equal. #######
</code></pre>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/c">
                                c
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
        </div>
    </body>
</html>
