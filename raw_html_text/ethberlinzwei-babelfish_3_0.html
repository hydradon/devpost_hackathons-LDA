<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div id="gallery">
                <ul>
                    <li>
                        <div class="flex-video">
                            <iframe allowfullscreen="allowfullscreen" allowscriptaccess="always" class="video-embed" frameborder="0" height="371" mode="transparent" src="https://www.youtube.com/embed/4grWV78uikE?enablejsapi=1&amp;hl=en_US&amp;rel=0&amp;start=&amp;version=3&amp;wmode=transparent" type="text/html" webkitallowfullscreen="true" width="660" wmode="transparent">
                            </iframe>
                        </div>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="829842" data-title="Front end" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/842/datas/original.png">
                            <img alt="ethberlinzwei-babelfish_3_0 &ndash; screenshot 1" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/842/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Front end
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="829848" data-title="Transcription" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/848/datas/original.png">
                            <img alt="ethberlinzwei-babelfish_3_0 &ndash; screenshot 2" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/848/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Transcription
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="829849" data-title="Request for transcription" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/849/datas/original.png">
                            <img alt="ethberlinzwei-babelfish_3_0 &ndash; screenshot 3" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/849/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Request for transcription
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="829851" data-title="Verification / adjudication metric" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/851/datas/original.png">
                            <img alt="ethberlinzwei-babelfish_3_0 &ndash; screenshot 4" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/851/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Verification / adjudication metric
                            </i>
                        </p>
                    </li>
                    <li class="text-center">
                        <a data-lightbox="829859" data-title="Reward payment transfer" href="https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/859/datas/original.png">
                            <img alt="ethberlinzwei-babelfish_3_0 &ndash; screenshot 5" class="software_photo_image image-replacement" onerror="this.onerror=null;this.src='https://devpost-challengepost.netdna-ssl.com/assets/defaults/thumbnail-placeholder-42bcab8d8178b413922ae2877d8b0868.gif';" src="//challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/829/859/datas/gallery.jpg">
                        </a>
                        <span class="expand-tag">
                            <i class="fas fa-expand">
                            </i>
                        </span>
                        <p>
                            <i>
                                Reward payment transfer
                            </i>
                        </p>
                    </li>
                </ul>
            </div>
            <div>
                <h1>
                    ethberlinzwei-babelfish_3_0
                </h1>
                <p>
                    EthBerlinZwei hackathon submission for a decentralized speech-to-text transcription service!
                </p>
                <p>
                    <a href="https://www.youtube.com/watch?v=4grWV78uikE&amp;feature=youtu.be" rel="nofollow">
                        Youtube screencast demo - full e2e
                    </a>
                </p>
                <h2>
                    Team
                </h2>
                <p>
                    <a href="https://github.com/MarcusJones" rel="nofollow">
                        Marcus
                    </a>
                    - Full stack
                </p>
                <p>
                    <a href="https://github.com/mtagda" rel="nofollow">
                        Magdalena
                    </a>
                    - Data Science
                </p>
                <h2>
                    Key innovations
                </h2>
                <ol>
                    <li>
                        Productionization of the open source DeepSearch speech transcription model
                    </li>
                    <li>
                        New similarity voting metric for automated reward for proper actors in the network
                    </li>
                    <li>
                        Intergration with the Ocean Protocol network for data ownership and incentivization
                    </li>
                    <li>
                        Concept development for decentralized dispute resolution
                    </li>
                    <li>
                        Proof of concept for generalized compute services orchestrated by Ethereum
                    </li>
                </ol>
                <h2>
                    Concept
                </h2>
                <p>
                    Speech to text transcription is the problem tackled in this project. This is furthermore a demo and hack for generalized compute and AI services which are orchestrated, incentivized, scaled, and secured by Ethereum smart contracts!
                </p>
                <p>
                    There exist several popular transcription services, for example, the
                    <a href="https://cloud.google.com/speech-to-text/" rel="nofollow">
                        Google Speech-To-Text
                    </a>
                    API.
                </p>
                <p>
                    But wait, are you willing to give away your sensitive audio speech files to a FANG company? What if you could be rewarded for providing your speech audio file? And what about competition and price discovery?
                </p>
                <p>
                    Project
                    <strong>
                        babelfish 3.0
                    </strong>
                    envisions and enables a mobile DApp which puts the power back in your hands.
                </p>
                <p>
                    As a user, you specify the price for your data (0 if you want to only purchase the transcription), the reward offered to the processor services, and the number of processor nodes. With these settings, your audio file is securely registered into the Ocean Protocol decentralized data registry.
                </p>
                <p>
                    Next, the processor nodes bid on offering the requested service. The chosen processors execute the transcription task, and submit (oracalize) their results to a Verifier smart contract.
                </p>
                <p>
                    The Verifier gets an input a list of results provided by the processors. It computest the Semantic Textual Similarity which is a base for obtaing fraud score for each transcription she/he got. If the fraud score of a transcription is lower than 0.5, the verifier considers it as fraud and returns 0, otherwise it outputs 1 indicating that the transcription task is verified.
                </p>
                <p>
                    After that, the Verifier distributes equally to all verified processors the amount of ETH she/he got from the client, keeping some amount of ETH as a reward for performing the verification. Finally, one of the verified text files is randomly chosen by the Verifier and sent to the Ocean Protocol. The client then gets the new asset which is a verified transcription of the initial audio file.
                </p>
                <h2>
                    Scope of the project
                </h2>
                <h3>
                    Prior work, libraries,
                </h3>
                <p>
                    This project makes use of the open source
                    <a href="https://oceanprotocol.com/" rel="nofollow">
                        Ocean Protocol
                    </a>
                    project to register and 'own' data assets.
                </p>
                <p>
                    This project uses the open source
                    <a href="https://github.com/mozilla/DeepSpeech" rel="nofollow">
                        DeepSpeech
                    </a>
                    library for transcription.
                </p>
                <h3>
                    Implemented
                </h3>
                <ul>
                    <li>
                        Interfacing to Ocean Protocol for
                        <em>
                            registration
                        </em>
                        of audio file via python client library.
                    </li>
                    <li>
                        Interfacing to Ocean Protocol for
                        <em>
                            download
                        </em>
                        of audio by processors
                    </li>
                    <li>
                        Transcription of audio using the open source
                        <a href="https://github.com/mozilla/DeepSpeech" rel="nofollow">
                            DeepSpeech
                        </a>
                        tensorflow library and pre-trained model.
                    </li>
                    <li>
                        Productionization of the transcription service with Flask
                    </li>
                    <li>
                        Simulation of the verifier service
                    </li>
                </ul>
                <h3>
                    Not implemented / WIP
                </h3>
                <ul>
                    <li>
                        Verifier smart contract not programmed or deployed
                    </li>
                    <li>
                        Full end-end integration (manual walkthrough, see below)
                    </li>
                </ul>
                <h2>
                    Project repo organization
                </h2>
                <p>
                    <a href="/frontend" rel="nofollow">
                        /frontend
                    </a>
                    contains the angular front-end interface for the DeepSearch transcription service
                </p>
                <p>
                    <a href="/backend" rel="nofollow">
                        /backend
                    </a>
                    contains the development and deployment of the backend transcription service with tensorflow
                </p>
                <p>
                    <a href="/scripts" rel="nofollow">
                        /scripts
                    </a>
                    contains the ocean protocol integration development
                </p>
                <h2>
                    Deep dive: Transcription service
                </h2>
                <p>
                    This project employs the DeepSpeech pre-trained model for speech transcription. DeepSpeech is an open source engine, using a model trained by machine learning techniques based on
                    <a href="https://arxiv.org/abs/1412.5567" rel="nofollow">
                        Baidu's Deep Speech research paper
                    </a>
                    . Project DeepSpeech uses Google's
                    <a href="https://www.tensorflow.org/" rel="nofollow">
                        TensorFlow
                    </a>
                    to serve predictions.
                </p>
                <p>
                    The model is built using a Recurrent Neural Network, with the architecture presented in the figure below.
                </p>
                <p>
                    <img alt="" data-canonical-url="images/deepspeechrnn.png" src="https://res.cloudinary.com/devpost/image/fetch/s--cXpDRz4q--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/v1/images/deepspeechrnn.png">
                </p>
                <p>
                    The model API has been wrapped with Python-Flask (back-end), and presented using a simple front-end (angular).
                </p>
                <p>
                    <img alt="" data-canonical-url="images/frontend.png" src="https://res.cloudinary.com/devpost/image/fetch/s--NtPPDFSR--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/v1/images/frontend.png">
                </p>
                <h2>
                    Deep dive: Verifier
                </h2>
                <p>
                    Once a Verifier gets a list of transcriptions from the processers, he/she encodes each text into high-dimensional vectors using
                    <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf" rel="nofollow">
                        Universal Sentence Encoder
                    </a>
                    .
                </p>
                <p>
                    <img alt="" data-canonical-url="images/emb.png" src="https://res.cloudinary.com/devpost/image/fetch/s--hoHjqW1y--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/v1/images/emb.png">
                </p>
                <p>
                    As next, the Verifier pairwise computes semantic similarity of the transcriptions which is a measure of the degree to which two pieces of text carry the same meaning. The semantic similarity of two sentences can be trivially computed as the inner product of the encodings. The following figure visualizes how the transcriptions are similar to each other where 1 is the highest and 0 the lowest semantic similarity score.
                </p>
                <p>
                    <img alt="" data-canonical-url="images/similarity_stt.png" src="https://res.cloudinary.com/devpost/image/fetch/s--EDOrUCd4--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/v1/images/similarity_stt.png">
                </p>
                <p>
                    Then, for each transcription the Verifier computes the Fraud Score which is simply the average semantic similarity score between one transcript and the remaining ones produced by other processors. In our example, the fraud scores would be the following: 0.774652, 0.77324998, 0.76432037, 0.75055325, 0.21729597. We consider each transcription with fraud score below 0.5 as not verified (returning 0) and all others as verified (returning 1). The Verifier returns a list made of zeros and ones (in our case: 1, 1, 1, 1, 0; this means that all but the last transcript are verified).
                </p>
                <h2>
                    Deep dive: End to end user story
                </h2>
                <p>
                    <a href="https://www.youtube.com/watch?v=4grWV78uikE&amp;feature=youtu.be" rel="nofollow">
                        Youtube screencast demo - full e2e
                    </a>
                </p>
                <h3>
                    Step 1: Client uses mobile phone to store audio
                </h3>
                <p>
                    <img alt="" data-canonical-url="images/speaking.png" src="https://res.cloudinary.com/devpost/image/fetch/s--Hdx9DwKg--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/v1/images/speaking.png">
                </p>
                <h3>
                    Step 2: Client registers audio into Ocean Protocol
                </h3>
                <pre class="language-bash"><code>python 0_publish_audio.py \
--url https://public-data-seeding.s3.eu-central-1.amazonaws.com/Test+Video+/testcoffeeshort.wav \
--price 10 \
--reward 50 \
--number-nodes 3

</code></pre>
                <p>
                    Command line API parameters;
                </p>
                <ul>
                    <li>
                        <code>
                            --url
                        </code>
                        the url of the asset
                    </li>
                    <li>
                        <code>
                            --price
                        </code>
                        the demanded price to download the asset (can be 0)
                    </li>
                    <li>
                        <code>
                            --reward
                        </code>
                        the offered reward for the processing job
                    </li>
                    <li>
                        <code>
                            --number-nodes
                        </code>
                        the number of processors requested for this job
                    </li>
                </ul>
                <p>
                    Increasing the number of processing nodes will increase the reliability of the transcription due to the voting mechanism, but can decrease the incentive for nodes to participate.
                </p>
                <p>
                    <a href="https://www.youtube.com/watch?v=cwy1cI4TBOo&amp;feature=youtu.be" rel="nofollow">
                        Youtube screencast demo - register audio
                    </a>
                </p>
                <h3>
                    Step 3: Processor nodes bid on the open job, and purchase the audio asset
                </h3>
                <pre class="language-bash"><code>python 1_processor_purchase_file.py \
--did did:op:f8252ea7a645475a9c519a241112ab292ff66b7d4a2049a5862b34bcfa507c30
</code></pre>
                <p>
                    Command line API parameters;
                </p>
                <ul>
                    <li>
                        <code>
                            --did
                        </code>
                        the Decentralized IDentifier of the registered audio asset to purchase
                    </li>
                </ul>
                <p>
                    <a href="https://www.youtube.com/watch?v=-7aANaSw7Xs&amp;feature=youtu.be" rel="nofollow">
                        Youtube screencast demo - consume audio
                    </a>
                </p>
                <h3>
                    Step 4: Processor nodes transcribe the audio, and submit results for verification
                </h3>
                <p>
                    Processor nodes download the asset from the Ocean Protocol and convert it into a text file using the deepspeech library. Then, they send the resulting transcription to the Verifier and wait for verification.
                </p>
                <h3>
                    Step 5: Verification process and transmission of final text transcription
                </h3>
                <p>
                    The Verifier gets the transcriptions from all the participating processor nodes and performs the step described previously (see
                    <em>
                        Deep dive: Verifier
                    </em>
                    ). After that, it distributes equally to all verified processors the amount of ETH she/he got from the client, keeping some amount of ETH as a reward for performing the verification. Finally, one of the verified text files  is randomly chosen by the Verifier and sent to the Ocean Protocol. The client then gets the new asset which is a verified transcription of the initial audio file.
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/amazon-web-services">
                                amazon-web-services
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/angular-js">
                                angular.js
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            tensorflow
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/MarcusJones/ethberlinzwei-babelfish_3_0" rel="nofollow" target="_blank" title="https://github.com/MarcusJones/ethberlinzwei-babelfish_3_0">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
