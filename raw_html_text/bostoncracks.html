<html>
    <head>
    </head>
    <body>
        <div class="large-9 columns" id="app-details-left">
            <div>
                <h1>
                    BostonCracks
                </h1>
                <p>
                    This project focused on:
                </p>
                <p>
                    <strong>
                        Challenge 1: Match Boston&rsquo;s Ground Truth Sidewalk Data
                    </strong>
                </p>
                <p>
                    <em>
                        Use Google Maps to try to replicate our sidewalk condition findings from 2014, to see if using this way is a viable option in the future.
                    </em>
                </p>
                <h2>
                    Sidewalk Segmentation
                </h2>
                <p>
                    If we want to be able to use Google Maps in order to replicate the city's 2014 findings we must be able to separate the sidewalk from any other aspects of a city scene. When using google street view this problem of separation becomes one very similar to the semantic image segmentation problem that self-driving cars face. With this realization, we decided to use a pre-trained
                    <a href="https://arxiv.org/pdf/1706.05587.pdf" rel="nofollow">
                        DeepLabV3
                    </a>
                    model to help us solve this problem. This pre-trained model fits directly with this problem as it was trained on the
                    <a href="https://www.cityscapes-dataset.com/" rel="nofollow">
                        Cityscapes Dataset
                    </a>
                    which translates very well to the urban environment of Boston.
                </p>
                <p>
                    What DeepLabV3 allows us to do is to take an image from google maps and get rid of everything but the sidewalk. For example:
                </p>
                <p>
                    <strong>
                        We first provide an image from google maps:
                    </strong>
                    <img alt="Image before DeepLabV3 segmentation" data-canonical-url="https://i.imgur.com/aeVgRMd.jpg" src="https://res.cloudinary.com/devpost/image/fetch/s--0xBxwJuc--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://i.imgur.com/aeVgRMd.jpg">
                </p>
                <p>
                    <strong>
                        Then the DeepLabV3 model segments the image:
                    </strong>
                    <img alt="Image after DeepLabV3" data-canonical-url="https://i.imgur.com/LoPdezX.jpg" src="https://res.cloudinary.com/devpost/image/fetch/s--qlbOnVrq--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://i.imgur.com/LoPdezX.jpg">
                </p>
                <p>
                    <strong>
                        Finally, we apply one last image pre-processing step and crop out everything but the sidewalk:
                    </strong>
                    <img alt="Final Image after all preprocessing" data-canonical-url="https://i.imgur.com/LX5Y10X.png" src="https://res.cloudinary.com/devpost/image/fetch/s--Y3LtgtST--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://i.imgur.com/LX5Y10X.png">
                </p>
                <p>
                    Now we can feed these isolated sidewalks into a classifier to determine if the sidewalk needs repairs!
                </p>
                <h2>
                    Sidewalk Damage Classification/Scoring
                </h2>
                <h4>
                    The dataset
                </h4>
                <p>
                    The possibility of finding a dataset containing damage scores for semantically segmented sidewalk was never even close to probable. Because of this, we were forced to make our own dataset, but because of Boston's StreetCaster project, we weren't running around blind. By extracting the latitude and longitude from the StreetCaster dataset we knew where to look for damaged sidewalks in Google Maps, and just about how damaged they should be. With this approach, we were successful in creating a small dataset (albeit tiny for ML) of labeled semantically segmented sidewalks.
                </p>
                <h4>
                    The architecture and transfer learning
                </h4>
                <p>
                    At the center of any meaningful machine learning model is the data, but once the data is collected we need an architecture to best utilize such data. We decided to choose ResNet18 as our model. Because we were limited by both time and amount of data we opted to use a pre-trained ResNet18 model and transfer its already learned features to this task of sidewalk damage scoring. This process, known as transfer learning allows us, in a sense, to teach a model that already knows how to see to simply make meaning of the things it detects in our dataset's images.
                </p>
                <h4>
                    The results
                </h4>
                <p>
                    After training for 100 epochs (iterations over the dataset) we saw the model successfully learn. However, it is questionable if the model was actually picking up on relevant features as the dataset the model was trained on is, as previously mentioned, tiny for an ML task. We do believe that our results lead to the conclusion that this is a worthwhile approach given additional time and resources.
                </p>
                <p>
                    <strong>
                        The loss:
                    </strong>
                    <img alt="Loss of our classification model over 100 epocs" data-canonical-url="https://i.imgur.com/6HQNGGx.png" src="https://res.cloudinary.com/devpost/image/fetch/s--H-q0Kxnq--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://i.imgur.com/6HQNGGx.png">
                </p>
                <h4>
                    Challenges we faced/where this could be improved
                </h4>
                <p>
                    The lack of a sizeable dataset is the greatest pressure point of this project. We originally wanted to programmatically pull Google Maps images from their API, but after looking into this option and finding out that the size and quality of the images return would not work well with both the segmentation and scoring model we decided our best option would be to manually create the dataset. We do believe that with more it is very possible to automate the dataset creation - at least partially (labeling may be tricky).
                </p>
                <p>
                    <strong>
                        Model Design in Summary:
                    </strong>
                    <img alt="Model Summary" data-canonical-url="https://i.imgur.com/BUpv1K8.png" src="https://res.cloudinary.com/devpost/image/fetch/s---N5eYC_H--/c_limit,f_auto,fl_lossy,q_auto:eco,w_900/https://i.imgur.com/BUpv1K8.png">
                </p>
            </div>
            <div class="" id="built-with">
                <h2>
                    Built With
                </h2>
                <ul class="no-bullet inline-list">
                    <li>
                        <span class="cp-tag">
                            cnns
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            deeplabv3
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/google-maps">
                                google-maps
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag">
                            keras
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/machine-learning">
                                machine-learning
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/python">
                                python
                            </a>
                        </span>
                    </li>
                    <li>
                        <span class="cp-tag recognized-tag">
                            <a href="https://devpost.com/software/built-with/pytorch">
                                pytorch
                            </a>
                        </span>
                    </li>
                </ul>
            </div>
            <nav class="app-links section">
                <h2>
                    Try it out
                </h2>
                <ul class="no-bullet" data-role="software-urls">
                    <li>
                        <a href="https://github.com/samc24/BostonCracks" rel="nofollow" target="_blank" title="https://github.com/samc24/BostonCracks">
                            <i class="ss-icon ss-link">
                            </i>
                            <span>
                                github.com
                            </span>
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </body>
</html>
