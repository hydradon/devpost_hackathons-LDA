{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset of all hackathon project descriptions\n",
    "df_apps = pd.read_csv(\"../dataset/proj_description_raw_local.csv\")\n",
    "\n",
    "# Rename a special case\n",
    "df_apps.project_url[df_apps.project_url == \"D:\\Research\\hackathon-devpost/raw_html_text\\\\aux1.html\"] = \"D:\\Research\\hackathon-devpost/raw_html_text\\\\aux.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the app name from the file name.\n",
    "# E.g, D:\\Research\\hackathon-devpost/raw_html_text\\1122bot.html => 1122bot\n",
    "df_apps[\"app\"] = df_apps[\"project_url\"].map(lambda x: x.split(\"\\\\\")[3].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve projects in hackathons that have ended\n",
    "df_proj_details = pd.read_csv(\"../dataset/all_project_ended_hack.csv\")\n",
    "df_proj_details[\"app\"] = df_proj_details[\"project_url\"].map(lambda x: x.split(\"/\")[4])\n",
    "df_proj_hackathon_pair = df_proj_details[[\"app\", \"hackathon_urls\", \"num_devs\"]]\n",
    "\n",
    "# match hackthon with project description\n",
    "txt_desc_hackathon_pair = df_apps.merge(df_proj_hackathon_pair, left_on=\"app\", right_on=\"app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize what section to process, and the number of topics to identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = [\n",
    "        \"txt_inspiration\",\n",
    "        \"txt_what_it_does\",\n",
    "        \"txt_how_we_built\", # How we built it, How I built it\n",
    "        \"txt_challenges\",\n",
    "        \"txt_accomplishment\",\n",
    "        \"txt_what_we_learned\",\n",
    "        \"txt_whats_next\"]\n",
    "\n",
    "section = text_map[6]\n",
    "num_topic = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with no data\n",
    "txt_desc_hackathon_pair.dropna(subset=[section], inplace=True)\n",
    "# Remove punctuation\n",
    "txt_desc_hackathon_pair['txt_processed'] = txt_desc_hackathon_pair[section].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Remove rows with no data again!\n",
    "txt_desc_hackathon_pair.dropna(subset=[section], inplace=True)\n",
    "# Convert to lower case\n",
    "txt_desc_hackathon_pair['txt_processed'] = txt_desc_hackathon_pair['txt_processed'].map(lambda x: x.lower())\n",
    "# Remove Emails\n",
    "txt_desc_hackathon_pair[\"txt_processed\"] = txt_desc_hackathon_pair[\"txt_processed\"].map(lambda x: re.sub('\\S*@\\S*\\s?', '', x))\n",
    "# Remove new line characters\n",
    "txt_desc_hackathon_pair[\"txt_processed\"] = txt_desc_hackathon_pair[\"txt_processed\"].map(lambda x: re.sub('\\s+', ' ', x))\n",
    "# Remove distracting single quotes\n",
    "txt_desc_hackathon_pair[\"txt_processed\"] = txt_desc_hackathon_pair[\"txt_processed\"].map(lambda x: re.sub(\"\\'\", \"\", x))\n",
    "# Remove empty description\n",
    "txt_desc_hackathon_pair = txt_desc_hackathon_pair[txt_desc_hackathon_pair.txt_processed != \"\"]\n",
    "txt_desc_hackathon_pair = txt_desc_hackathon_pair[txt_desc_hackathon_pair.txt_processed != \" \"]\n",
    "\n",
    "# Convert to list\n",
    "data = txt_desc_hackathon_pair[\"txt_processed\"].tolist()\n",
    "\n",
    "# Extract processed text - hackathon pairs\n",
    "processed_txt_hackathon_pair = txt_desc_hackathon_pair[[\"txt_processed\", \"project_url\", \"hackathon_urls\", \"num_devs\"]]\n",
    "\n",
    "# Convert file path to HTML path\n",
    "processed_txt_hackathon_pair[\"project_url\"] = processed_txt_hackathon_pair[\"project_url\"].map(lambda x: \"https://devpost.com/software/\" + x.split(\"\\\\\")[3].replace(\".html\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, remove stopwords, make bigrams/trigrams, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Mallet LDA model\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "os.environ['MALLET_HOME'] = 'D:\\Research\\hackathon-devpost\\Python_scripts\\mallet-2.0.8'\n",
    "mallet_path = 'D:\\Research\\hackathon-devpost\\Python_scripts\\mallet-2.0.8\\\\bin\\mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, id2word=id2word,\n",
    "                                            num_topics=num_topic, \n",
    "                                            iterations=500, # default = 1000                                            \n",
    "                                            random_seed=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Finding the dominant topic in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Finding the dominant topic in each sentence\n",
    "def format_topics_sentences(ldamodel, corpus, texts, project_url, hackathons, num_devs):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]): # length of data\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents, pd.Series(project_url), pd.Series(hackathons), pd.Series(num_devs)], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data, \n",
    "                                                  project_url=processed_txt_hackathon_pair[\"project_url\"].tolist(),\n",
    "                                                  hackathons=processed_txt_hackathon_pair[\"hackathon_urls\"].tolist(),\n",
    "                                                  num_devs=processed_txt_hackathon_pair[\"num_devs\"].tolist())\n",
    "df_topic_sents_keywords.columns = ['Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', \"Project_Url\", \"Hackathon_Url\", \"Num_Devs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the dominant topic (and its proportion) for each project\n",
    "\n",
    "Example: [dominant_15_topics_txt_challenges_doc.csv](../dataset/dominant_15_topics_txt_challenges_doc.csv) -> For each of the project, show which topic (among the 15 topics) is the dominant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE------ Reading hackathon type and splice the data\n",
    "# Read hackathon type file\n",
    "hackathon_type = pd.read_csv(\"../dataset/hackathons_top_bot_20.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', \"Project_Url\", \"Hackathon_Url\", \"Num_Devs\"]\n",
    "\n",
    "# Format the contribution to 4 decimal places\n",
    "df_dominant_topic[\"Topic_Perc_Contrib\"] = df_dominant_topic[\"Topic_Perc_Contrib\"].map('{:,.4f}'.format)\n",
    "\n",
    "# Save before splitting hackathons => # NOTE still ok!\n",
    "df_dominant_topic_sorted = df_dominant_topic.sort_values([\"Dominant_Topic\", \"Topic_Perc_Contrib\"], ascending=[True, False])\n",
    "\n",
    "# Combined with hackathon type\n",
    "df_dominant_topic_sorted[\"Hackathon_Url\"] = df_dominant_topic_sorted[\"Hackathon_Url\"].map(lambda a: a.split(\"||\"))\n",
    "df_dominant_topic_sorted = df_dominant_topic_sorted.explode(\"Hackathon_Url\")\n",
    "df_dominant_topic_sorted = df_dominant_topic_sorted.merge(hackathon_type, left_on=\"Hackathon_Url\", right_on=\"hackathon_url\", how=\"left\")\n",
    "\n",
    "output = \"./dominant_\" + str(num_topic) + \"_topics_\" + section + \"_doc.csv\"\n",
    "if os.path.exists(output):\n",
    "    os.remove(output)\n",
    "df_dominant_topic_sorted.to_csv(output, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find how many hackathons where a topic is a dominant topic in at least one project\n",
    "\n",
    "Example: [dominant_15_topics_txt_challenges_hackathons_count.csv](../dataset/dominant_15_topics_txt_challenges_hackathons_count.csv) -> For each of the 15 topics in \"Challenges\" section, show the number of hackathons where at least one project has this topic as the dominant topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split hackathon strings xyz||abc to 2 rows\n",
    "df_dominant_topic[\"Hackathon_Url\"] = df_dominant_topic[\"Hackathon_Url\"].map(lambda a: a.split(\"||\"))\n",
    "df_dominant_topic = df_dominant_topic.explode(\"Hackathon_Url\")\n",
    "\n",
    "temp = df_dominant_topic.drop(columns = [\"Document_No\", \"Text\", \"Topic_Perc_Contrib\", \"Keywords\", \"Project_Url\"])\n",
    "temp.drop_duplicates(inplace=True) # Rationale: 2 projects, 1 topic, 1 hackathon => 2 rows => only count the topic as one \n",
    "\n",
    "# Find the counts of dominant topics across hackathon\n",
    "topic_hackathons_count = temp[\"Dominant_Topic\"].value_counts().to_frame()\n",
    "topic_hackathons_count = pd.DataFrame({'Topic_Num'      : topic_hackathons_count.index, \n",
    "                                       'Hackathon_Count': topic_hackathons_count.Dominant_Topic})\n",
    "# Write to file\n",
    "output = \"./dominant_\" + str(num_topic) + \"_topics_\" + section + \"_hackathons_count.csv\"\n",
    "if os.path.exists(output):\n",
    "    os.remove(output)\n",
    "topic_hackathons_count.to_csv(output, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the dominant topics in popular and non-popular hackathons\n",
    "\n",
    "Example: [dorminant_15_topics_txt_challenges_Yes_popular_hack](dorminant_15_topics_txt_challenges_Yes_popular_hack.csv) -> For each of the 15 topics, show the number of projects in popular hackathons (hackathons that have the top 20% number of submissions) where this topic is dominant. Also show the percentage contribution of each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO need to fix this: currently it shows all topics for all hackathons\n",
    "## => split to only Yes hackathon, and No hackathon\n",
    "df_topic_sents_keywords[\"Hackathon_Url\"] = df_topic_sents_keywords[\"Hackathon_Url\"].map(lambda a: a.split(\"||\"))\n",
    "df_topic_sents_keywords = df_topic_sents_keywords.explode(\"Hackathon_Url\")\n",
    "df_topic_sents_keywords = df_topic_sents_keywords.merge(hackathon_type, left_on=\"Hackathon_Url\", right_on=\"hackathon_url\", how=\"left\")\n",
    "\n",
    "#split into popular and non-popular hackathons\n",
    "df_topic_sents_keywords_popular_yes = df_topic_sents_keywords[df_topic_sents_keywords.popular == \"Yes\"]\n",
    "df_topic_sents_keywords_popular_no  = df_topic_sents_keywords[df_topic_sents_keywords.popular == \"No\"]\n",
    "\n",
    "def process_topic_distribution(df_topic_sents_keywords, ispopular, num_hackathons):\n",
    "    ### Topic distribution across documents\n",
    "    # Number of Documents for Each Topic\n",
    "    topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "    topic_counts_df = topic_counts.to_frame()\n",
    "    topic_counts_df = pd.DataFrame({'Topic_Num'    : topic_counts_df.index, \n",
    "                                    'Project_Count': topic_counts_df.Dominant_Topic})\n",
    "\n",
    "    # Percentage of Documents for Each Topic\n",
    "    topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "    topic_contribution_df = topic_contribution.to_frame()\n",
    "    topic_contribution_df = pd.DataFrame({'Topic_Num'    : topic_contribution_df.index, \n",
    "                                          'Topic_Contrib': topic_contribution_df.Dominant_Topic})\n",
    "    topic_contribution_df[\"Topic_Contrib\"] = topic_contribution_df[\"Topic_Contrib\"].map('{:,.4f}'.format) # Format the contribution to 4 decimal places\n",
    "\n",
    "\n",
    "    # Do hackathon counts\n",
    "    temp = df_topic_sents_keywords.drop(columns = [\"Text\", \"Topic_Perc_Contrib\", \"Keywords\", \"Project_Url\"])\n",
    "    temp.drop_duplicates(inplace=True)\n",
    "    # Find the counts of dorminant topics across hackathon\n",
    "    topic_hackathons_count = temp[\"Dominant_Topic\"].value_counts()\n",
    "    topic_hackathons_count_df = topic_hackathons_count.to_frame()\n",
    "    topic_hackathons_count_df = pd.DataFrame({'Topic_Num'      : topic_hackathons_count_df.index, \n",
    "                                              'Hackathon_Count': topic_hackathons_count_df.Dominant_Topic})\n",
    "    # Percentage of hackathon for Each Topic\n",
    "    hackathon_contrib = round(topic_hackathons_count/num_hackathons, 4)\n",
    "    hackathon_contrib_df = hackathon_contrib.to_frame()\n",
    "    hackathon_contrib_df = pd.DataFrame({'Topic_Num'        : hackathon_contrib_df.index, \n",
    "                                         'Hackathon_Contrib': hackathon_contrib_df.Dominant_Topic})\n",
    "    hackathon_contrib_df[\"Hackathon_Contrib\"] = hackathon_contrib_df[\"Hackathon_Contrib\"].map('{:,.4f}'.format)\n",
    "\n",
    "    # Topic Number and Keywords\n",
    "    topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Keywords']]\n",
    "\n",
    "    merge = topic_counts_df.merge(topic_num_keywords.rename(columns={\"Dominant_Topic\":\"Topic_Num\"}), left_index=True, right_on=\"Topic_Num\").drop_duplicates()\n",
    "    final = merge.merge(topic_contribution_df, left_on=\"Topic_Num\", right_index=True)\n",
    "    final = final.merge(topic_hackathons_count_df, left_on=\"Topic_Num\", right_index=True)\n",
    "    final = final.merge(hackathon_contrib_df, left_on=\"Topic_Num\", right_index=True)\n",
    "    # Calculate percentage of hackathons that this topic was dominant\n",
    "    # final_final = final.merge(topic_hackathons_count, left_on=\"Topic_Num\", right_index=True)\n",
    "    # final_final[\"Perc_Hackathon\"] = final_final[\"Hackathon_Count\"] / 2195.0\n",
    "    # final_final[\"Perc_Hackathon\"] = final_final[\"Perc_Hackathon\"].map('{:,.2f}'.format)\n",
    "\n",
    "    final_final = final\n",
    "    final_final.drop(columns = [\"Topic_Num_x\", \"Topic_Num_y\"], inplace=True)\n",
    "\n",
    "    # Write to file \n",
    "    output = \"./dominant_\" + str(num_topic) + \"_topics_\" + section + \"_\" + ispopular + \"_popular_hack.csv\"\n",
    "    if os.path.exists(output):\n",
    "        os.remove(output)\n",
    "    final_final.to_csv(output, encoding='utf-8-sig', index=False)\n",
    "\n",
    "process_topic_distribution(df_topic_sents_keywords_popular_yes, \"Yes\", len(hackathon_type)/2)\n",
    "process_topic_distribution(df_topic_sents_keywords_popular_no, \"No\", len(hackathon_type)/2)"
   ]
  }
 ]
}